<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>曹焱杰的笔记 | 星想事成</title><meta name="author" content="星想事成"><meta name="copyright" content="星想事成"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="曹焱杰的笔记  一、手写简单的卷积神经网络我们将基于nn.Module为里面添加层（卷积、池化、线性等）来构建一个下面这个简单的卷积神经网络模型  Convolution Layers（卷积层）1torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride&#x3D;1, padding&#x3D;0, dilation&#x3D;1, groups&#x3D;1, bia">
<meta property="og:type" content="article">
<meta property="og:title" content="曹焱杰的笔记">
<meta property="og:url" content="http://hello123candy.github.io/2024/01/12/%E6%9B%B9%E7%84%B1%E6%9D%B0%E7%9A%84%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="星想事成">
<meta property="og:description" content="曹焱杰的笔记  一、手写简单的卷积神经网络我们将基于nn.Module为里面添加层（卷积、池化、线性等）来构建一个下面这个简单的卷积神经网络模型  Convolution Layers（卷积层）1torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride&#x3D;1, padding&#x3D;0, dilation&#x3D;1, groups&#x3D;1, bia">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hello123candy.github.io/picture/33.jpg">
<meta property="article:published_time" content="2024-01-12T06:00:46.000Z">
<meta property="article:modified_time" content="2024-01-12T07:36:30.161Z">
<meta property="article:author" content="星想事成">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hello123candy.github.io/picture/33.jpg"><link rel="shortcut icon" href="/favicon.png"><link rel="canonical" href="http://hello123candy.github.io/2024/01/12/%E6%9B%B9%E7%84%B1%E6%9D%B0%E7%9A%84%E7%AC%94%E8%AE%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '曹焱杰的笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-12 15:36:30'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/7.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">41</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/./picture/33.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="星想事成"><span class="site-name">星想事成</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">曹焱杰的笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-12T06:00:46.000Z" title="发表于 2024-01-12 14:00:46">2024-01-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-12T07:36:30.161Z" title="更新于 2024-01-12 15:36:30">2024-01-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/PyTorch/">PyTorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">18.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>72分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="曹焱杰的笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="曹焱杰的笔记"><a href="#曹焱杰的笔记" class="headerlink" title="曹焱杰的笔记"></a>曹焱杰的笔记</h1><hr>

<h2 id="一、手写简单的卷积神经网络"><a href="#一、手写简单的卷积神经网络" class="headerlink" title="一、手写简单的卷积神经网络"></a>一、手写简单的卷积神经网络</h2><p>我们将基于nn.Module为里面添加层（卷积、池化、线性等）来构建一个下面这个简单的卷积神经网络模型</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E5%258D%25B7%25E7%25A7%25AF%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C.png" alt="卷积神经网络"></p>
<h4 id="Convolution-Layers（卷积层）"><a href="#Convolution-Layers（卷积层）" class="headerlink" title="Convolution Layers（卷积层）"></a>Convolution Layers（卷积层）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, padding_mode=<span class="string">&#x27;zeros&#x27;</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>pytorch提供了许多的具体实现，比如nn.Conv1d,nn.Convi2d，nn.Conv3d等等，</p>
<p>我们图像识别是2维的，所以我们使用的具体实现是nn.Conv2d</p>
<p>以下是官方文档中提供的Conv2d的参数，不过其中有一部分不是很常用，在这里不做讲解。</p>
<p><strong>in_channels</strong>：输入数据的通道数。对于RGB图像，通道数为3；对于灰度图像或单通道数据，通道数为1。</p>
<p><strong>out_channels</strong>：输出数据的通道数（或者也可以说是滤波器&#x2F;卷积核的数量）。每个滤波器都会产生一个输出通道。</p>
<p><strong>kernel_size</strong>：卷积核（滤波器）的大小。可以是一个整数以表示正方形核的边长，也可以是一个元组 <code>(H, W)</code> 表示高度和宽度分别为 H 和 W 的非正方形核。</p>
<p><strong>stride</strong>：步幅。它控制着卷积核在输入数据上滑动的步长。默认值为1，表示卷积核每次只移动1个像素；如果设为2，则每次移动2个像素。</p>
<p><strong>padding</strong>：填充。指定在输入数据的边缘周围添加多少层填充0。这将有助于保持输出特征图的大小与输入特征图一致。</p>
<p><strong>dilation</strong>：扩张率。用于控制卷积核中各个元素之间的间距。默认值为1，表示元素之间相邻；设置大于1的值将增加它们之间的距离。</p>
<p><strong>groups</strong>：分组卷积。将输入和输出通道分成若干组，在每组上独立进行卷积运算，并且各组之间不交互。</p>
<p><strong>bias</strong>：是否使用偏置项。如果设置为True，则会学习一个额外的偏置权重来调整每个滤波器输出值。</p>
<p><strong>padding_mode</strong>：填充模式。填充时所采用的方式，默认为’zeros’即使用0进行填充。</p>
<p><strong>device和dtype</strong>：指定了tensor所在设备和数据类型，默认情况下不需要手动指定。</p>
<p>为了更直观的感受，我们首先新建一个5x5的张量，再新建一个3x3的卷积核，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span>=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                    [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">                    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                    [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">kernel = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                       [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">                       [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br></pre></td></tr></table></figure>

<p>因为需要用conv2d，conv2d限制了input的格式，所以input要按照它来进行格式转换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将输入张量input重塑为一个形状为 (1, 1, 5, 5) 的张量，表示单个样本、单通道、大小为 5x5 的特征图</span></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将卷积核张量kernel重塑为一个形状为 (1, 1, 3, 3) 的张量，表示一个单通道到单通道的大小为3x3的卷积核</span></span><br><span class="line">kernel = torch.reshape(kernel, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<p>最后print输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output=F.conv2d(<span class="built_in">input</span>,kernel)<span class="comment">#stride=1，表示步长为1</span></span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[<span class="number">10</span>, <span class="number">11</span>,  <span class="number">8</span>],</span><br><span class="line">          [<span class="number">10</span>, <span class="number">13</span>,  <span class="number">9</span>],</span><br><span class="line">          [<span class="number">13</span>,  <span class="number">8</span>,  <span class="number">3</span>]]]])</span><br></pre></td></tr></table></figure>

<p>工作原理：蓝色为input，阴影部分为卷积核，青色为output</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E5%258D%25B7%25E7%25A7%25AF%25E6%25A0%25B8%25E5%25B7%25A5%25E4%25BD%259C%25E5%258E%259F%25E7%2590%2586.jpg" alt="卷积核工作原理"></p>
<p>当我们没有规定padding和strides时，padding默认是0，strides为1</p>
<p><strong>padding</strong></p>
<p>当我们规定了padding时，它会对我们的图片进行填充，当padding设置为1时，默认在最外面加一圈并填充0，卷积之后的结果也会发生改变。此时input和output大小相同</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/padding=1.png" alt="padding=1"></p>
<p>当我们规定padding为2时，output的大小又会发生改变，此时output的大小为6x6</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/padding=2.png" alt="padding=2"></p>
<p>代码展示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span>=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                    [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">                    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                    [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">kernel = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                       [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">                       [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span>=torch.reshape(<span class="built_in">input</span>,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">kernel = torch.reshape(kernel,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 padding 参数为1  的情况下进行二维卷积运算，输出形状会与原始输入相同</span></span><br><span class="line">output = F.conv2d(<span class="built_in">input</span>,kernel,padding=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 padding 参数为2 的情况下进行二维卷积运算，输出形状将包含更多边界填充</span></span><br><span class="line">output = F.conv2d(<span class="built_in">input</span>,kernel,padding=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">7</span>,  <span class="number">3</span>],</span><br><span class="line">          [ <span class="number">5</span>, <span class="number">10</span>, <span class="number">11</span>,  <span class="number">8</span>,  <span class="number">4</span>],</span><br><span class="line">          [ <span class="number">3</span>, <span class="number">10</span>, <span class="number">13</span>,  <span class="number">9</span>,  <span class="number">4</span>],</span><br><span class="line">          [ <span class="number">7</span>, <span class="number">13</span>,  <span class="number">8</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">          [ <span class="number">6</span>,  <span class="number">8</span>,  <span class="number">7</span>,  <span class="number">6</span>,  <span class="number">4</span>]]]])</span><br><span class="line">tensor([[[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">4</span>,  <span class="number">4</span>,  <span class="number">2</span>,  <span class="number">5</span>,  <span class="number">2</span>],</span><br><span class="line">          [ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">7</span>,  <span class="number">3</span>,  <span class="number">0</span>],</span><br><span class="line">          [ <span class="number">1</span>,  <span class="number">5</span>, <span class="number">10</span>, <span class="number">11</span>,  <span class="number">8</span>,  <span class="number">4</span>,  <span class="number">1</span>],</span><br><span class="line">          [ <span class="number">0</span>,  <span class="number">3</span>, <span class="number">10</span>, <span class="number">13</span>,  <span class="number">9</span>,  <span class="number">4</span>,  <span class="number">2</span>],</span><br><span class="line">          [ <span class="number">1</span>,  <span class="number">7</span>, <span class="number">13</span>,  <span class="number">8</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">2</span>],</span><br><span class="line">          [ <span class="number">1</span>,  <span class="number">6</span>,  <span class="number">8</span>,  <span class="number">7</span>,  <span class="number">6</span>,  <span class="number">4</span>,  <span class="number">1</span>],</span><br><span class="line">          [ <span class="number">2</span>,  <span class="number">5</span>,  <span class="number">4</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">1</span>]]]])</span><br></pre></td></tr></table></figure>

<p><strong>strides</strong></p>
<p>当strides为2时，卷积核每次移动2步（上下和左右都是）</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/strides.jpg" alt="strides"></p>
<p>代码展示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span>=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                    [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">                    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">                    [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                    [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">kernel = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                       [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">                       [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span>=torch.reshape(<span class="built_in">input</span>,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">kernel = torch.reshape(kernel,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">output=F.conv2d(<span class="built_in">input</span>,kernel,stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">Pooling layers（池化层）</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[10,  8],</span><br><span class="line">          [13,  3]]]])</span><br></pre></td></tr></table></figure>

<p>了解完这些，我们就可以在nn.Module里面添加自己的卷积层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义神经网络模块类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">module</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 初始化函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 调用父类的初始化方法</span></span><br><span class="line">        <span class="built_in">super</span>(module, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义第一个卷积层: 输入通道数为3（彩色图像），输出通道数为6，</span></span><br><span class="line">        <span class="comment"># 核大小为6x6，步长为1，填充为0（&#x27;valid&#x27;卷积）</span></span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">6</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 应用第一个卷积层</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 返回经过卷积后的特征图</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化自定义模块对象</span></span><br><span class="line">module = module()</span><br><span class="line"><span class="built_in">print</span>(module)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">module(</span><br><span class="line">  (conv1): Conv2d(<span class="number">3</span>, <span class="number">6</span>, kernel_size=(<span class="number">6</span>, <span class="number">6</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br><span class="line"><span class="comment"># - 输入通道数(`in_channels`)：3</span></span><br><span class="line"><span class="comment">#   - 这意味着该层期待从前一个层（或输入数据）接收含有3个通道的数据，例如RGB图像。</span></span><br><span class="line"><span class="comment"># - 输出通道数(`out_channels`)：6</span></span><br><span class="line"><span class="comment">#   - 表示该卷积层将会输出6个不同的特征映射(Feature Maps)。</span></span><br><span class="line"><span class="comment"># - 内核/过滤器大小(`kernel_size`)：(6, 6)</span></span><br><span class="line"><span class="comment">#   - 指明每个过滤器或内核在进行卷积操作时覆盖输入数据的区域大小为6x6个像素值。</span></span><br><span class="line"><span class="comment"># - 步长(`stride`)：(1, 1)</span></span><br><span class="line"><span class="comment">#   - 设置卷积时滑动窗口移动的步长。这里它每次在垂直和水平方向上都移动1个像素点。</span></span><br></pre></td></tr></table></figure>

<p>然后我们就可以往模型里面输入图片，来查看经过卷积之后图片的尺寸。</p>
<p>在下面这个例子中，<code>imgs</code>是从CIFAR10数据集加载的一批图像，而<code>outputs</code>是这批图像通过神经网络模块（包含一个卷积层）处理后得到的结果。我们这里就不解释为什么尺寸会发生变化，我把公式放在这里，大家有兴趣的可以自己去学习一下：</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/output%25E5%2585%25AC%25E5%25BC%258F.png" alt="output公式"></p>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需的库</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载CIFAR10数据集，将图像转换为张量，并在本地不存在时下载数据集。</span></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(</span><br><span class="line">    <span class="string">&quot;./dataset&quot;</span>,  <span class="comment"># 数据集保存路径</span></span><br><span class="line">    train=<span class="literal">False</span>,  <span class="comment"># 指定加载的是测试集（train=False表示不是训练集）</span></span><br><span class="line">    transform=torchvision.transforms.ToTensor(),  <span class="comment"># 转换PIL图片为张量</span></span><br><span class="line">    download=<span class="literal">True</span>  <span class="comment"># 如果数据集未下载，则开始下载</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个数据加载器，批量大小设置为64。</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化上面定义的Module类</span></span><br><span class="line">module = module()</span><br><span class="line"></span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;../logs&quot;</span>)</span><br><span class="line">step=<span class="number">0</span></span><br><span class="line"><span class="comment"># 迭代数据加载器中的每一批数据</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data  <span class="comment"># 解包获取图像和标签</span></span><br><span class="line"></span><br><span class="line">    outputs = module(imgs)  <span class="comment"># 对图像应用神经网络模型得到输出结果</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)      <span class="comment"># 打印原始图像张量的形状（尺寸）</span></span><br><span class="line">    <span class="built_in">print</span>(outputs.shape)   <span class="comment"># 打印模型输出的张量形状（尺寸）</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>,imgs,step)</span><br><span class="line">    torch.reshape(outputs,([-<span class="number">1</span>,<span class="number">3</span>,<span class="number">30</span>,<span class="number">30</span>]))</span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>,outputs,step)</span><br><span class="line"></span><br><span class="line">    step += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>尺寸输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])<span class="comment">#imgs.shape</span></span><br><span class="line">torch.Size([<span class="number">64</span>, <span class="number">6</span>, <span class="number">30</span>, <span class="number">30</span>])<span class="comment">#outputs.shape</span></span><br></pre></td></tr></table></figure>

<p>然后我们在Tensorboard中看一下经过卷积的图片究竟变成了什么样子：</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E5%258D%25B7%25E7%25A7%25AF%25E5%25B1%2582%25E8%25BE%2593%25E5%2585%25A5.png" alt="卷积层输入"><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E5%258D%25B7%25E7%25A7%25AF%25E5%25B1%2582%25E8%25BE%2593%25E5%2587%25BA.png" alt="卷积层输出"></p>
<h4 id="Pooling-layers（池化层）"><a href="#Pooling-layers（池化层）" class="headerlink" title="Pooling layers（池化层）"></a>Pooling layers（池化层）</h4><p>我们讲解MaxPool2d（最大池化）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MaxPool2d(kernel_size, stride=<span class="literal">None</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, return_indices=<span class="literal">False</span>, ceil_mode=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>然后是官方分档提供的一部分参数，然后自己的一些解释：</p>
<ul>
<li><strong>kernel_size</strong>: 这个参数定义了池化窗口的大小。也就是在输入特征图上滑动并应用池化操作的区域尺寸。它可以是一个单一整数（表示正方形窗口），或者是一个整数元组来表示矩形窗口。</li>
<li><strong>stride</strong>: 步长定义了窗口在移动过程中每次跳过多少像素&#x2F;元素。默认情况下，如果没有指定步长，则通常与相同，意味着窗口在每次移动时不会重叠。步长也可以通过一个整数或一个元组来指定。</li>
<li><strong>padding</strong>: 填充是添加到输入特征图两侧边界的额外像素&#x2F;元素数量，以控制输出特征图的空间尺寸。填充通常用于保持特征图大小或减少边缘效应。这个值可以是单一整数（对所有边界应用相同大小填充）或者一个元组。</li>
<li><strong>ceil_mode</strong>: 当设置为<code>True</code>时，在计算输出尺寸时将使用向上取整(ceil)而不是向下取整(floor)方法来处理边界情况，这可能会导致输出特征图稍微大一点。</li>
</ul>
<p><strong>工作原理：</strong></p>
<p>input和池化核</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E6%25B1%25A0%25E5%258C%2596.png" alt="池化"></p>
<p>最大池化顾名思义就是取池化核中最大的那个值，也就是2，然后根据stride的定义，我们不定义步长时，步长与池化核相同，移动三步</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E6%25B1%25A0%25E5%258C%2596%25E7%25AC%25AC%25E4%25B8%2580%25E6%25AD%25A5.png" alt="池化第一步"></p>
<p>那么在这种情况下，我们是否取值呢，我们要根据ceil_mode来判断</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E6%25B1%25A0%25E5%258C%2596%25E7%25AC%25AC%25E4%25BA%258C%25E9%2583%25A8.png" alt="池化第二部"></p>
<p>当Ceil_mode为True时，取最大值，当为False时，不取最大值</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/Ceil%25E2%2580%2594model.png" alt="Ceil—model"></p>
<p>现在往我们的nn.Module中添加池化层。</p>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MaxPool2d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个输入张量，其值如下所示。</span></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1.0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                      [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                      [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                      [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span> ,<span class="number">1</span>]])</span><br><span class="line"><span class="comment"># 调整张量的形状以匹配期望的四维格式 (N,C,H,W)</span></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span> ,<span class="number">5</span> ,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个PyTorch模块，其中包含一个最大池化层。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">module</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 在类初始化方法中定义模块结构。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(module,self).__init__()</span><br><span class="line">        <span class="comment"># 创建一个最大池化层，kernel_size为3，启用ceil_mode。</span></span><br><span class="line">        self.maxpool1 = MaxPool2d(kernel_size=<span class="number">3</span> ,ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播函数。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化上面定义的module模块对象。</span></span><br><span class="line">module = module()</span><br><span class="line"><span class="comment"># 将输入数据透过定义好的最大池化层进行前向传播计算。</span></span><br><span class="line">output=module(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># 打印输出结果。</span></span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[<span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">          [<span class="number">2.</span>, <span class="number">1.</span>]]]])</span><br></pre></td></tr></table></figure>

<p>然后我们加入TensorBoard看一下图片经过池化之后的样子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(dataset,batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(module,self).__init__()</span><br><span class="line">        self.maxpool1=MaxPool2d(kernel_size=<span class="number">3</span>,ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=self.maxpool1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">module= module()</span><br><span class="line"></span><br><span class="line">writer=SummaryWriter(<span class="string">&quot;../logs_Max&quot;</span>)</span><br><span class="line"></span><br><span class="line">step=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line"></span><br><span class="line">    imgs,targets = data</span><br><span class="line">    outputs = module(imgs)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="built_in">print</span>(outputs.shape)</span><br><span class="line">    </span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>,imgs,step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># outputs=torch.reshape(outputs,(-1,3,30,30))</span></span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>,outputs,step)</span><br><span class="line"></span><br><span class="line">    step += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>input：</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E6%25B1%25A0%25E5%258C%2596%25E8%25BE%2593%25E5%2585%25A5.png" alt="池化输入"></p>
<p>output：</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E6%25B1%25A0%25E5%258C%2596%25E8%25BE%2593%25E5%2587%25BA.png" alt="池化输出"></p>
<h4 id="Non-linear-Activations-weighted-sum-nonlinearity）非线性函数"><a href="#Non-linear-Activations-weighted-sum-nonlinearity）非线性函数" class="headerlink" title="Non-linear Activations (weighted sum, nonlinearity）非线性函数"></a>Non-linear Activations (weighted sum, nonlinearity）非线性函数</h4><p>非线性：引入非线性函数来增加网络复杂度，使得神经网络可以学习并近似更复杂的函数。常见例子包括ReLU、sigmoid和tanh等激活函数。</p>
<p>非线性层通常指包含非线性激活函数的层。</p>
<p>非线性层的主要作用就是用来为图片增加一些非线性特征，主要的好处有：</p>
<ol>
<li><strong>增加表达能力</strong>：如果没有非线性激活函数，无论多少个神经网络层堆叠起来，整个网络仍然只能表示线性函数。通过添加非线性函数，网络可以学习到更复杂的模式。</li>
<li><strong>防止过拟合</strong>：某些类型的非线性激活函数（如ReLU及其变种）具有散射特征——它们不会对所有输入都产生反应。这意味着在训练过程中，网络中一部分节点会“关闭”，从而提供了一种内置形式的正则化。</li>
</ol>
<p>直接看图片区别：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">dataset=torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">dataloader=DataLoader(dataset,batch_size=<span class="number">64</span>)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Jiege</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Jiege,self).__init__()</span><br><span class="line">        self.sigmoid1=Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        output=self.sigmoid1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">jiege=Jiege()</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;./logs_sigmoid&quot;</span>)</span><br><span class="line">step=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs,targets = data</span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>,imgs,global_step=step)</span><br><span class="line">    output=jiege(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>,output,step)</span><br><span class="line">    step+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>input：</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E9%259D%259E%25E7%25BA%25BF%25E6%2580%25A7%25E5%25B1%2582%25E8%25BE%2593%25E5%2585%25A5.png" alt="非线性层输入"></p>
<p>output：</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E9%259D%259E%25E7%25BA%25BF%25E6%2580%25A7%25E5%25B1%2582%25E8%25BE%2593%25E5%2587%25BA.png" alt="非线性层输出"></p>
<h4 id="Linear-Layers（线性层）"><a href="#Linear-Layers（线性层）" class="headerlink" title="Linear Layers（线性层）"></a>Linear Layers（线性层）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Linear(in_features, out_features, bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>在神经网络中，线性层（也常被称为全连接层或密集层）起着基础和关键的作用。线性层主要是执行一个线性变换，把输入数据映射到新的空间。</p>
<p>它接收输入并应用线性变换，将每个输入与对应的权重相乘并求和，就比如说，我们的（64，3，32，32）大小的input，总共会返回196608个特征的输入向量，</p>
<p>如果使用PyTorch中的 <code>Linear</code> 类。这个线性层会将输入数据从196608维映射到10维。换句话说，它将具有196608个特征的输入向量转换为具有10个特征的输出向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 PyTorch 的神经网络模块 nn</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过继承 nn.Module 来定义自定义的神经网络模块类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 构造函数：初始化这个类的实例</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 调用基类（父类）的构造器来正确地初始化它</span></span><br><span class="line">        <span class="built_in">super</span>(Module, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 添加一个线性层作为该模块的属性。</span></span><br><span class="line">        <span class="comment"># 这个线性层会将输入数据从196608维转变成10维输出。</span></span><br><span class="line">        <span class="comment"># 这通常用于将高维数据映射到低维，例如在分类任务中。</span></span><br><span class="line">        self.Linear1 = nn.Linear(in_features=<span class="number">196608</span>, out_features=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播方法：定义输入数据如何通过网络并返回输出。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># 将输入数据通过上面定义的线性层，并返回其输出。</span></span><br><span class="line">        <span class="comment"># 线性层通过学习得到的权重和偏置进行线性变换。</span></span><br><span class="line">        output = self.Linear1(<span class="built_in">input</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 返回输出，即被转换成10维的张量。</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<p><strong>Flattening（展平）</strong></p>
<p>Flattening（展平）是将多维数组变换为一维数组的过程。在深度学习中，在经过诸如卷积层之后，当我们需要将数据输入到全连接层（也称为线性层）之前，通常会进行展平操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加入Flatten层：转换之前卷积的输出，从多维特征图到一维向量。</span></span><br><span class="line"><span class="comment"># 这通常在准备将数据传给全连接/线性层前做。</span></span><br><span class="line">nn.Flatten()</span><br></pre></td></tr></table></figure>

<p><strong>Sequential（序列）</strong></p>
<p>Sequential是一个容器，它按照它们被添加的顺序包含不同的模块（例如层）。当给定输入数据时，Sequential 会自动处理将数据通过其所有子模块的过程。这使得构建神经网络更为简洁，因为您可以轻松地将层堆叠在一起，而不必编写显式的前向传播代码。</p>
<p> Sequential 容器允许你将网络组件定义为列表，并自动管理它们之间的数据流。每个组件（或称作 “layer”）都执行其专门的运算，并且输出会作为下一组件（或 “layer”）的输入。</p>
<p>不使用序列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(CustomNetwork, self).__init__()</span><br><span class="line">        self.layer1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.layer2 = nn.Linear(<span class="number">5</span>, <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model_without_sequential = CustomNetwork()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model_without_sequential)</span><br></pre></td></tr></table></figure>

<p>使用序列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个包含两个全连接层的序列化模型</span></span><br><span class="line">model_with_sequential = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">10</span>, <span class="number">5</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">5</span>, <span class="number">2</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model_with_sequential)</span><br></pre></td></tr></table></figure>

<p>使用<strong>nn.Sequential</strong>可以使得代码更加简洁直观，非常适合那些结构规则、顺序执行的网络。而不使用它，则需要编写更多代码，但你可以设计出更加复杂和灵活的网络结构。</p>
<h4 id="手动实现一个卷积神经网络"><a href="#手动实现一个卷积神经网络" class="headerlink" title="手动实现一个卷积神经网络"></a><strong>手动实现一个卷积神经网络</strong></h4><p>在经过这些学习之后，我们现在可以完成一开始的目标</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个神经网络模块继承自nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Module,self).__init__()  <span class="comment"># 调用父类构造函数</span></span><br><span class="line">        <span class="comment"># 使用Sequential来定义一个模型序列</span></span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            <span class="comment"># 第一层是卷积层，输入通道数为3（如RGB图像），输出通道数为32，卷积核大小为5x5，边缘填充为2（padding）</span></span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 接着是最大池化层，窗口大小为2x2</span></span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 再来一个相同配置的卷积层和最大池化层组合；</span></span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 最后一个卷积层输出通道数增加到64，其余配置不变。</span></span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 使用Flatten()把多维数据展平成一维数据；</span></span><br><span class="line">            Flatten(),</span><br><span class="line">            <span class="comment"># 然后是两个线性层；第一个全连接层接收展平后的向量，并输出64个特征；</span></span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">             <span class="comment"># 最后一个线性层把64个特征转化成10个特征，比如可以对应10分类问题。</span></span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义前向传播过程</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=self.model1(x)   <span class="comment"># 输入数据通过model1处理得到结果x</span></span><br><span class="line">        <span class="keyword">return</span> x           <span class="comment"># 返回处理结果</span></span><br><span class="line"></span><br><span class="line">module=Module()          <span class="comment"># 创建模块实例</span></span><br><span class="line"><span class="built_in">print</span>(module)            <span class="comment"># 打印模块结构，这句的作用是用来查看模型结构</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#输出一个input，用来验证模型是否正确</span></span><br><span class="line">input1 = torch.ones((<span class="number">64</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))   <span class="comment"># 创建输入张量：批次大小为64，3个颜色通道，每个通道大小为32x32的图像。</span></span><br><span class="line">output = module(input1)              <span class="comment"># 将输入张量传入模型进行前向计算获取输出。</span></span><br><span class="line"><span class="built_in">print</span>(output.shape)                  <span class="comment"># 打印输出张量的形状。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我们还可以通过TensorBoard来查看这个网络模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writer=SummaryWriter(<span class="string">&quot;../log&quot;</span>)</span><br><span class="line">writer.add_graph(module,input1)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E6%25A8%25A1%25E5%259E%258B%25E6%259F%25A5%25E7%259C%258B.png" alt="模型查看"></p>
<h2 id="二、实现卷积网络二分类"><a href="#二、实现卷积网络二分类" class="headerlink" title="二、实现卷积网络二分类"></a><strong>二、实现卷积网络二分类</strong></h2><h4 id="损失函数和反向传播"><a href="#损失函数和反向传播" class="headerlink" title="损失函数和反向传播"></a>损失函数和反向传播</h4><h5 id="为什么使用损失函数？"><a href="#为什么使用损失函数？" class="headerlink" title="为什么使用损失函数？"></a><strong>为什么使用损失函数？</strong></h5><p>我们要不断完善模型，就必然要经过以下步骤：</p>
<ol>
<li><strong>前向传播</strong>（Forward Propagation）：模型接受训练数据并计算出预测结果。</li>
<li><strong>计算损失</strong>（Loss Calculation）：损失函数比较预测结果和真实值，输出表示它们不一致程度的数值，即损失。</li>
<li><strong>反向传播</strong>（Backpropagation）：根据损失函数计算出来的梯度，模型调整内部参数以减小预测结果和真实值之间的差距。</li>
<li><strong>参数更新</strong>（Parameter Update）：使用优化算法（如随机梯度下降SGD），模型根据梯度对参数进行更新。（优化器）</li>
<li><strong>迭代优化</strong>（Iterative Optimization）：这个过程在多个训练周期或批次上反复执行，直到损失最小化或达到某种性能标准为止。（循环）</li>
</ol>
<p>反正目标总是使得模型通过学习降低损失值，从而提高其在特定任务上的预测精确性。</p>
<p><strong>二元交叉熵损失函数</strong></p>
<p>本书旨在实战，所以就讲一个损失函数，剩下的有想要了解的可以去学习一下。</p>
<p>二元交叉熵损失函数(Binary Cross-Entropy Loss)，也称为对数损失（Log Loss），是在二分类问题中常用的一种损失函数。它衡量的是模型预测概率和真实标签之间的差异。</p>
<p>假设我们有一个二分类任务，其中标签 <code>y</code> 可以是 0 或 1，模型预测出的概率为 <code>p</code>（表示样本属于类别 1 的概率）。那么，二元交叉熵损失可以定义为：<br>$$<br>L(y, p) &#x3D; -[y log(p) + (1 - y) log(1 - p)]<br>$$<br>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定我们有3个样本，带有概率预测和相应的真实标签。</span></span><br><span class="line">inputs = torch.tensor([<span class="number">0.7</span>, <span class="number">0.3</span>, <span class="number">0.5</span>], requires_grad=<span class="literal">True</span>)    <span class="comment"># 示例模型预测（概率）</span></span><br><span class="line">targets = torch.tensor([<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>])                          <span class="comment"># 真实标签</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># BCELoss 需要输入为 [0,1] 范围内的概率，并且目标为包含类别标签（0或1）的同尺寸张量。</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 BCE 损失值</span></span><br><span class="line">loss = criterion(inputs, targets)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;计算得到的二元交叉熵损失值：: <span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 让我们执行一个 backward 传递来计算梯度（演示目的）</span></span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">计算得到的二元交叉熵损失值：: <span class="number">0.4688323736190796</span></span><br></pre></td></tr></table></figure>



<h5 id="如何选择损失函数？"><a href="#如何选择损失函数？" class="headerlink" title="如何选择损失函数？"></a>如何选择损失函数？</h5><p> 在选择损失函数时，应该考虑以下几点： </p>
<ol>
<li>能够很好地反映出数据的关键特性。</li>
<li>使用适当的方法使不同的特征在相同的尺度上比较，避免某些特征过于主导结果。</li>
<li>根据实验结果调整模型参数，确保模型能够有效区分不同类别。 </li>
<li>如果需要，可以结合多个损失函数来更好地评估模型表现。 </li>
<li>在设计损失函数时考虑任务的具体需求，帮助模型专注于最重要的信息。</li>
</ol>
<h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><p>在机器学习和深度学习中，优化器是用来更新神经网络权重以减少损失函数值的算法。它们利用损失函数计算出的梯度信息来调整模型参数，目标是让模型更好地拟合训练数据。</p>
<p>以下是一些优化器，大家可以根据需要来切换使用：</p>
<ol>
<li><strong>SGD (Stochastic Gradient Descent)</strong>: 这是最基本的优化方法，通过查看少量数据来更新模型一点点。</li>
<li><strong>Momentum</strong>: 为了更快地更新和减少震荡，Momentum保留了之前更新方向的一部分，在新的更新中加入这个信息。</li>
<li><strong>Adagrad</strong>: 自动调整每个参数的学习率，使得频繁修改的参数变动小一些，不那么频繁修改的变动大一些。</li>
<li><strong>RMSprop</strong>: 解决Adagrad随时间慢下来太快的问题，持续调整学习率但不让其降得太低。</li>
<li><strong>Adam (Adaptive Moment Estimation)</strong>: 结合了Momentum和RMSprop的思想，并且考虑过去梯度（速度）及其平方（规模）来调整学习速率。</li>
<li><strong>AdamW</strong>: 是对Adam算法的一个小改进，在权重衰减（类似于正则化）方面做得更好。</li>
<li><strong>Nadam</strong>: 结合了Nesterov 动量版本的SGD和Adam, 加上所谓先知步骤预测未来梯度行为来做出更好地权重调整决策。</li>
</ol>
<p>当你初始化一个优化器时，你需要将模型参数传递给它，并设置其他超参数如学习率。例如，在PyTorch中使用SGD优化器代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>



<h4 id="二分类的实现"><a href="#二分类的实现" class="headerlink" title="二分类的实现"></a>二分类的实现</h4><p>我们首先来对数据进行打标，而不是用pytorch的类加载器。为什么不用pytorch的类加载器而是手动打标。</p>
<p>因为数据标注是在机器学习中建立准确模型的基础工作。为了更好的理解这一点，并培养他们在这方面的技能，我们从实践出发，通过亲身体验来感受整个过程。亲自动手对图片或文本进行标注，查看模型在使用不同质量的数据时会有怎样的表现差异，有助于明白自己工作的价值与影响。</p>
<p>总之，数据标注不仅仅是一个技术性任务，而且是一个认识到其对最终机器学习模型性能影响深远的过程。</p>
<p>首先，定义一个函数load_img，并创建一个空数组 <code>shuzu</code> 来存储图片数据和标签。</p>
<ol>
<li>使用 <code>os.walk</code> 函数递归地遍历给定路径下的所有目录和子目录。遍历到每个目录时，都会得到三个值：当前目录的路径(<code>dirpath</code>)、该目录下子目录名列表(<code>dirnames</code>)、以及该目录下文件名列表(<code>files</code>)。</li>
<li>对于 <code>files</code> 中的每个文件名 (<code>item</code>)：<ul>
<li>检查这个文件是不是图片类型（具有扩展名 <code>.jpg</code>, <code>.JPG</code>, 或 <code>.png</code>）。）。</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_img</span>():</span><br><span class="line">    shuzu=[]</span><br><span class="line">    	<span class="keyword">for</span> dirpath, dirnames, files <span class="keyword">in</span> os.walk(<span class="string">r&#x27;picture\train&#x27;</span>):</span><br><span class="line">        	<span class="keyword">for</span> item <span class="keyword">in</span> files:</span><br><span class="line">            	<span class="keyword">if</span> item.find(<span class="string">&quot;.jpg&quot;</span>) != -<span class="number">1</span> <span class="keyword">or</span> item.find(<span class="string">&quot;.JPG&quot;</span>) != -<span class="number">1</span> <span class="keyword">or</span> item.find(<span class="string">&quot;.png&quot;</span>) != -<span class="number">1</span>:</span><br></pre></td></tr></table></figure>

<p>如果发现了符合条件的图片文件：</p>
<ul>
<li>使用 <code>cv2.imread</code> 加载图片。</li>
<li>使用 <code>cv2.resize</code> 将图片大小调整为 112x112 像素。</li>
<li>将 BGR 格式转换成 RGB 格式。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">picture = cv2.imread(dirpath+<span class="string">&quot;/&quot;</span>+item)</span><br><span class="line">resized_image = cv2.resize(picture, (<span class="number">112</span>, <span class="number">112</span>))</span><br><span class="line">img_pil = Image.fromarray(cv2.cvtColor(resized_image,cv2.COLOR_BGR2RGB))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>并且通过 <code>transforms.ToTensor()</code> 转换成 PyTorch 张量 (tensor)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_pil = toTensor(img_pil)<span class="comment">#toTensor需要在函数外面定义</span></span><br></pre></td></tr></table></figure>

<p><code>transforms.ToTensor()</code> 是PyTorch库中的一个转换操作，它将PIL图像或者NumPy ndarray转换为PyTorch的Tensor对象。这句话大概在导包之后，因为使用PyTorch框架时，经常需要将原始的图像数据类型转换为Tensor，因为Tensor是PyTorch用于高效计算的主要数据结构。	</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">toTensor = transforms.ToTensor() </span><br></pre></td></tr></table></figure>

<p>最后根据父目录名称确定标签：</p>
<ul>
<li>如果父目录 (<code>dirpath</code>) 包含字符 “0”，则假设该图片属于类别 “0” 并指定其标签为 <code>[0, 1]</code>（通常代表二分类问题中的第一类）。</li>
<li>否则，假设该图片属于类别 “1” 并指定其标签为 <code>[1, 0]</code>（代表二分类问题中的第二类）。</li>
</ul>
<ol>
<li>将包含预处理过的图像张量和相关联标签的元组添加到数组 <code>shuzu</code> 中。</li>
<li>函数结束时返回填充好的数组 <code>shuzu</code>。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> dirpath.find(<span class="string">r&quot;0&quot;</span>) !=-<span class="number">1</span>:</span><br><span class="line">  </span><br><span class="line">                shuzu.append((img_pil,<span class="number">0</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">                shuzu.append((img_pil,<span class="number">1</span>))</span><br><span class="line">                </span><br><span class="line"><span class="keyword">return</span> shuzu</span><br></pre></td></tr></table></figure>

<p>引用自定义函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shuzu=load_img()</span><br></pre></td></tr></table></figure>

<p>引用模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model=SiameseNetwork()<span class="comment">#SiameseNetwork是网上开源的一个模型主要用于解决那些需要比较输入数据对相似性或关系的任务。</span></span><br></pre></td></tr></table></figure>

<p>定义损失函数和优化器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.BCELoss() <span class="comment">#二分类</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.0005</span>) <span class="comment"># 优化器</span></span><br></pre></td></tr></table></figure>

<p>因为我们的input是一个四维变量,但是我们现在是一个三维的，所以我们需要再给他一个维度，并且我们在这个过程中还可以让数组乱序，以此来达到每次训练不一样</p>
<ol>
<li><p><code>random.shuffle(shuzu)</code>： 这行代码使用 <code>random.shuffle()</code> 函数将名为 <code>shuzu</code> 的列表内的元素顺序打乱（洗牌）。</p>
</li>
<li><p>初始化空列表 <code>add=[]</code>： 创建一个新的空列表，命名为 <code>add</code>，用于存储稍后抽取的元素。</p>
</li>
<li><p>抽取随机元素并添加到 add 列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">random.shuffle(shuzu)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line">    random_integer = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(shuzu)-<span class="number">1</span>)</span><br><span class="line">    add.append(shuzu[random_integer])</span><br></pre></td></tr></table></figure>

<p>这段代码通过循环16次，每次从 <code>shuzu</code> 中用 <code>random.randint()</code> 随机选择一个索引值，并将对应索引位置上的元素添加至 <code>add</code> 列表。</p>
</li>
<li><p>初始化两个空列表：input&#x3D;[] 和 biaoqian&#x3D;[]。 这两个列表分别用来存储从 <code>add</code> 中抽取出来的数据部分和标签（label）部分。</p>
</li>
<li><p>分离输入数据和标签并填充到各自列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> add:</span><br><span class="line">    <span class="built_in">input</span>.append(i[<span class="number">0</span>])</span><br><span class="line">    biaoqian.append(i[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>在这个循环中， <code>add</code> 列表中每个元素都是一个包含两部分的结构，其中第一部分是要处理的输入数据（在此添加到了 <code>input</code> 列表），第二部分是相应的标签（加入到了 <code>biaoqian</code> 列表）。</p>
</li>
<li><p>将输入数据转换为张量并堆叠，将标签转换为张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.stack(<span class="built_in">input</span>, dim=<span class="number">0</span>)</span><br><span class="line">b = torch.tensor(biaoqian)</span><br><span class="line">   </span><br></pre></td></tr></table></figure>

<p>使用PyTorch库将input列表中所有输入数据堆叠起来形成一个新的多维张量a。</p>
<p>同样地，将biaoqian列表转换成一个新的张量b。</p>
</li>
</ol>
<p>源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">random.shuffle(shuzu)</span><br><span class="line">    add=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line"></span><br><span class="line">        random_integer = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(shuzu)-<span class="number">1</span>)</span><br><span class="line">        add.append(shuzu[random_integer])</span><br><span class="line">    <span class="built_in">input</span>=[]</span><br><span class="line">    biaoqian=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> add:</span><br><span class="line">        <span class="built_in">input</span>.append(i[<span class="number">0</span>])</span><br><span class="line">        biaoqian.append(i[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        a = torch.stack(<span class="built_in">input</span>, dim=<span class="number">0</span>)</span><br><span class="line">        b = torch.tensor(biaoqian)</span><br></pre></td></tr></table></figure>

<p>开启训练模式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.train()<span class="comment">#开启训练模式</span></span><br></pre></td></tr></table></figure>

<p>开始训练 ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">img, label= Variable(a).cuda(), Variable(b).cuda()<span class="comment">#没有gpu就把.cuda()去掉</span></span><br><span class="line"><span class="comment">#每次迭代都要把梯度置零</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">output1 = model(img.<span class="built_in">float</span>())</span><br><span class="line"></span><br><span class="line">loss_contrastive= criterion(output1, label.<span class="built_in">float</span>())   <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">loss_contrastive.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化参数</span></span><br><span class="line">optimizer.step()</span><br><span class="line">val_loss=loss_contrastive.item()</span><br></pre></td></tr></table></figure>

<p>整体源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> SiameseNetwork</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">toTensor = transforms.ToTensor() </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_img</span>():</span><br><span class="line">    shuzu=[]</span><br><span class="line">    <span class="keyword">for</span> dirpath, dirnames, files <span class="keyword">in</span> os.walk(<span class="string">r&#x27;val&#x27;</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">if</span> item.find(<span class="string">&quot;.jpg&quot;</span>) != -<span class="number">1</span> <span class="keyword">or</span> item.find(<span class="string">&quot;.JPG&quot;</span>) != -<span class="number">1</span> <span class="keyword">or</span> item.find(<span class="string">&quot;.png&quot;</span>):</span><br><span class="line">                picture = cv2.imread(dirpath+<span class="string">&quot;/&quot;</span>+item)</span><br><span class="line">                resized_image = cv2.resize(picture, (<span class="number">112</span>, <span class="number">112</span>))</span><br><span class="line">                <span class="comment">#print(resized_image.shape)</span></span><br><span class="line">                img_pil = Image.fromarray(cv2.cvtColor(resized_image,cv2.COLOR_BGR2RGB))</span><br><span class="line">                img_pil = toTensor(img_pil)</span><br><span class="line">                <span class="comment"># img_pil= transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ], std = [ 0.229, 0.224, 0.225 ])(img_pil) #处理数据【-1，1】之间</span></span><br><span class="line">                <span class="keyword">if</span> dirpath.find(<span class="string">r&quot;0&quot;</span>) !=-<span class="number">1</span>:</span><br><span class="line">                    <span class="comment">#print(&quot;0文件夹&quot;)</span></span><br><span class="line">                    shuzu.append((img_pil,<span class="number">0</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment">#print(&quot;1文件夹&quot;)</span></span><br><span class="line">                    shuzu.append((img_pil,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">                </span><br><span class="line">    <span class="keyword">return</span> shuzu</span><br><span class="line">                </span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">shuzu=load_img()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用random.shuffle()函数打乱列表的顺序</span></span><br><span class="line"></span><br><span class="line">model=SiameseNetwork()</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;当前设备是:&#x27;</span>,device)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.BCELoss() <span class="comment">#二分类</span></span><br><span class="line"><span class="comment">#criterion =nn.BCEWithLogitsLoss()</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.0005</span>) <span class="comment"># 优化器</span></span><br><span class="line">min_loss=<span class="number">5000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(min_loss):</span><br><span class="line"></span><br><span class="line">    random.shuffle(shuzu)</span><br><span class="line">    add=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line"></span><br><span class="line">        random_integer = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(shuzu)-<span class="number">1</span>)</span><br><span class="line">        add.append(shuzu[random_integer])</span><br><span class="line">    <span class="built_in">input</span>=[]</span><br><span class="line">    biaoqian=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> add:</span><br><span class="line">        <span class="built_in">input</span>.append(i[<span class="number">0</span>])</span><br><span class="line">        biaoqian.append(i[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        a = torch.stack(<span class="built_in">input</span>, dim=<span class="number">0</span>)</span><br><span class="line">        b = torch.tensor(biaoqian)</span><br><span class="line">   </span><br><span class="line">    model.train()<span class="comment">#开启训练模式</span></span><br><span class="line">    <span class="comment"># img0 和标签数据 label0 封装为 PyTorch 变量，并将它们移动到 GPU（cuda）上进行处理。没有GPU把.cuda()去掉。</span></span><br><span class="line">    img, label= Variable(a).cuda(), Variable(b).cuda()</span><br><span class="line">    <span class="comment">#每次迭代都要把梯度置零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    output1 = model(img.<span class="built_in">float</span>())</span><br><span class="line"></span><br><span class="line">    loss_contrastive= criterion(output1, label.unsqueeze(<span class="number">1</span>).<span class="built_in">float</span>())   <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    loss_contrastive.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 优化参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    val_loss=loss_contrastive.item()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> q %<span class="number">10</span>== <span class="number">0</span> :</span><br><span class="line">    <span class="built_in">print</span>(loss_contrastive.item())</span><br><span class="line">    <span class="keyword">if</span> loss_contrastive.item()&lt;<span class="number">0.1</span>:</span><br><span class="line">        n=n+<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n&gt;<span class="number">100</span>:</span><br><span class="line">            exit()</span><br><span class="line">        torch.save(model.state_dict(), <span class="built_in">str</span>(val_loss)+<span class="string">&quot;.pkl&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;保存权值结束程序！&quot;</span>)</span><br></pre></td></tr></table></figure>

<p> SiameseNetwork模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SiameseNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SiameseNetwork, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义卷积层 a，输入通道数为 3，输出通道数为 32，卷积核大小为 3</span></span><br><span class="line">        self.a = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        self.b = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.c = nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.d = nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.e = nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义激活函数 ReLU，inplace 参数设置为 True，表示在原地执行激活操作</span></span><br><span class="line">        self.ReLU = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义最大池化层 MaxPool2d，池化窗口大小为 2</span></span><br><span class="line">        self.MaxPool2d = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义展平层 Flatten，用于将多维数据展平成一维</span></span><br><span class="line">        self.Flatten = nn.Flatten()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义全连接层 fc1，包括一个线性层（输入为 512，输出为 1）和一个 Sigmoid 激活函数</span></span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">            <span class="comment"># nn.ReLU(),</span></span><br><span class="line">            <span class="comment"># nn.Linear(128, 1),</span></span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line">       </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x1</span>):</span><br><span class="line">        </span><br><span class="line">        x1 = self.a(x1)</span><br><span class="line">        x1 = self.ReLU(x1)</span><br><span class="line">        x1 = self.MaxPool2d(x1)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        x1 = self.b(x1)</span><br><span class="line">        x1 = self.ReLU(x1)</span><br><span class="line">        x1 = self.MaxPool2d(x1)</span><br><span class="line"></span><br><span class="line">        x1 = self.c(x1)</span><br><span class="line">        x1 = self.ReLU(x1)</span><br><span class="line">        x1 = self.MaxPool2d(x1)</span><br><span class="line"></span><br><span class="line">        x1 = self.d(x1)</span><br><span class="line">        x1 = self.ReLU(x1)</span><br><span class="line">        x1 = self.MaxPool2d(x1)</span><br><span class="line">        <span class="comment">#print(x1.shape)</span></span><br><span class="line">        </span><br><span class="line">        x1 = self.e(x1)</span><br><span class="line">        x1 = self.ReLU(x1)</span><br><span class="line">        x1 = self.MaxPool2d(x1)</span><br><span class="line">        <span class="comment">#print(x1.shape)</span></span><br><span class="line">        </span><br><span class="line">        x1 =self.Flatten(x1)</span><br><span class="line"></span><br><span class="line">        x1=self.fc1(x1)</span><br><span class="line">        <span class="keyword">return</span> x1</span><br></pre></td></tr></table></figure>



<h4 id="模型如何保存"><a href="#模型如何保存" class="headerlink" title="模型如何保存"></a>模型如何保存</h4><p>在经过以上的操作之后我们会得到一个较为理想的模型，可是模型的训练需要时间和大量的计算资源，所以，模型训练完成之后，保存是十分重要的。</p>
<p>有两种保存模型的方法，这里我们只讲1种，也就是官方推荐的只保存模型参数的方法</p>
<p>这种方式会把模型参数放在一个字典里，然后保存字典来随时调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="built_in">str</span>(val_loss)+<span class="string">&quot;.pkl&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>torch.save</code>: 这是PyTorch框架提供的函数，用来将对象序列化并保存到文件中。</li>
<li><code>model.state_dict()</code>: 这是一个方法，用于获取当前模型（<code>model</code>）的状态字典（state dictionary）。状态字典包含了模型所有层的参数（例如：权重和偏置）。</li>
<li><code>str(val_loss)+&quot;.pkl&quot;</code>: 这里首先将变量<code>val_loss</code>（通常指代验证集上计算得到的损失值）转换为字符串形式，然后在其后添加拓展名”.pkl”。最终得到的字符串被用作文件名。例如，如果<code>val_loss</code>等于0.032654762268066406，则文件名就会是”0.032654762268066406.pkl”。</li>
</ul>
<p>这样就保存了一个以<code>val_loss</code>命名的模型。记得加在二分类代码的后面哦，要不然就白跑了。</p>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>卷积神经网络的测试也十分重要。CNN作为一种深度学习模型，应用广泛。而对CNN的测试能确保模型的性能和准确率满足特定的应用需求。</p>
<p>所以，在任何将要投入实际应用场景之前，对卷积神经网络进行测试是至关重要的一步，确保该网络能够理想地执行所需任务，也证明其稳健性和可靠性。</p>
<p>调用保存的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_path =<span class="string">r&#x27;0.032654762268066406.pkl&#x27;</span></span><br><span class="line">model.load_state_dict(torch.load(model_path))<span class="comment">#加载保存在文件中的权重</span></span><br></pre></td></tr></table></figure>

<p>由于这个读取模型的过程中即使文件不存在也不会报错，为了保证它能够成功读取，我们可以加一个判断</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model_path =<span class="string">r&#x27;0.032654762268066406.pkl&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(model_path):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;开始加载模型&#x27;</span>)</span><br><span class="line">    model.load_state_dict(torch.load(model_path))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;没有找到模型，退出！&#x27;</span>)</span><br><span class="line">    exit()    </span><br></pre></td></tr></table></figure>

<p>对测试图片进行处理，同训练图片。</p>
<p>然后，开启测试模式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>() <span class="comment">#测试模式</span></span><br></pre></td></tr></table></figure>

<p>之后我们就该对返回的结果进行判断</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">if dirpath.find(r&quot;0&quot;) !=-1:</span><br><span class="line">   </span><br><span class="line">                biaoqian.append(0)</span><br><span class="line">            else:</span><br><span class="line"> </span><br><span class="line">                biaoqian.append(1)</span><br><span class="line">            结果=model(img_pil)</span><br><span class="line">            if 结果&gt;0.5:</span><br><span class="line">                结果=1</span><br><span class="line">            else:</span><br><span class="line">                结果=0</span><br><span class="line">            if biaoqian[num-1]==结果:</span><br><span class="line">                sum=sum+1</span><br><span class="line">print(sum/num*100) </span><br></pre></td></tr></table></figure>

<p>源码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from PIL import Image</span><br><span class="line">import cv2</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">import torch</span><br><span class="line">import random</span><br><span class="line">from ai999 import SiameseNetwork</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">import shutil</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def dutupian():</span><br><span class="line">    shuzu=[]</span><br><span class="line">    biaoqian=[]</span><br><span class="line">    num=0</span><br><span class="line">    sum=0</span><br><span class="line">    for dirpath, dirnames, files in os.walk(r&#x27;val&#x27;):</span><br><span class="line">        print(f&#x27;发现文件夹:&#123;dirpath&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">        for item in files:</span><br><span class="line">            num+=1</span><br><span class="line">            #print (item)</span><br><span class="line">            if item.find(&quot;.jpg&quot;) != -1 or item.find(&quot;.JPG&quot;) != -1 or item.find(&quot;.png&quot;):</span><br><span class="line">                #print(dirpath+&quot;/&quot;+item)</span><br><span class="line">                picture = cv2.imread(dirpath+&quot;/&quot;+item) #趋向于0 0.0005332682630978525</span><br><span class="line">                try:</span><br><span class="line">                    resized_image = cv2.resize(picture, (112, 112))</span><br><span class="line">                except:</span><br><span class="line">                    print(dirpath+&quot;/&quot;+item,&quot;错误&quot;) </span><br><span class="line">                      </span><br><span class="line">    </span><br><span class="line">                img_pil = Image.fromarray(cv2.cvtColor(resized_image,cv2.COLOR_BGR2RGB))</span><br><span class="line">                img_pil = toTensor(img_pil)</span><br><span class="line">                img_pil = img_pil.unsqueeze(0)</span><br><span class="line">                img_pil= Variable(img_pil).cuda()</span><br><span class="line">                if dirpath.find(r&quot;0&quot;) !=-1:</span><br><span class="line">                    biaoqian.append(0)</span><br><span class="line">                else:</span><br><span class="line">                    biaoqian.append(1)</span><br><span class="line">                结果=model(img_pil)</span><br><span class="line">                if 结果&gt;0.5:</span><br><span class="line">                    结果=1</span><br><span class="line">                else:</span><br><span class="line">                    结果=0</span><br><span class="line">                if biaoqian[num-1]==结果:</span><br><span class="line">                    sum=sum+1</span><br><span class="line">    print(sum/num*100)    </span><br><span class="line">    return shuzu            </span><br><span class="line">toTensor = transforms.ToTensor() </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 使用random.shuffle()函数打乱列表的顺序</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model=SiameseNetwork()</span><br><span class="line">print(model)</span><br><span class="line"></span><br><span class="line">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line">print(&#x27;当前设备是:&#x27;,device)</span><br><span class="line">model.to(device)</span><br><span class="line">model_path =r&#x27;9.392573701916263e-06.pkl&#x27;</span><br><span class="line"></span><br><span class="line">if os.path.exists(model_path):</span><br><span class="line">    print(&#x27;开始加载模型&#x27;)</span><br><span class="line">    model.load_state_dict(torch.load(model_path))</span><br><span class="line">else:</span><br><span class="line">    print(&#x27;没有找到模型，退出！&#x27;)</span><br><span class="line">    exit()    </span><br><span class="line">model.eval() #测试模式</span><br><span class="line"></span><br><span class="line">shuzu=dutupian()</span><br></pre></td></tr></table></figure>



<h2 id="三、实现卷积网络多分类"><a href="#三、实现卷积网络多分类" class="headerlink" title="三、实现卷积网络多分类"></a><strong>三、实现卷积网络多分类</strong></h2><p>卷积神经网络（CNN）在解决二分类和多分类问题时，网络架构的基本组成部分例如卷积层、池化层等通常相同。然而，在网络的最终阶段即输出层，以及与之相关的激活函数和损失函数选择上存在一些主要区别。以下是二分类和多分类任务在这些方面的对比。</p>
<h4 id="输出层的区别"><a href="#输出层的区别" class="headerlink" title="输出层的区别"></a>输出层的区别</h4><ul>
<li><strong>二分类问题：</strong> 对于二元类别判断，一般只需一个神经元作为输出层。该神经元的输出值表示样本预测为正类（通常指标签为1的类别）的概率。</li>
<li><strong>多分类问题：</strong> 对于多类别判定（N个类），输出层则需要N个神经元，每个代表模型预测样本属于一个特定类别的概率。</li>
</ul>
<p>上一节我们讲二分类的时候使用的是SiameseNetwork网络，大家可以看一下源码。</p>
<p>SiameseNetwork的网络结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.fully_connect1 = torch.nn.Linear(flat_shape, <span class="number">512</span>)</span><br><span class="line">self.fully_connect2 = torch.nn.Linear(<span class="number">512</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>我们实现的CIFAR -10模型的网络结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Linear(<span class="number">1024</span>, <span class="number">64</span>)</span><br><span class="line">Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p>可以看到，最后的输出一个是1，一个是10，也就代表了一个是二分类，一个是十分类。因为神经元的返回值可以说是一个类别的概率，当你为二分类时，判断神经元的返回值是趋近于0还是1来判断是0类还是1类；当你为十分类时，需要模型判断每一个类别的概率，然后我们取最大值来判断属于哪个类。</p>
<h4 id="激活函数的区别"><a href="#激活函数的区别" class="headerlink" title="激活函数的区别"></a>激活函数的区别</h4><ul>
<li><strong>二分类问题：</strong> 输出层通常采用Sigmoid函数，将值压缩到0到1之间，表示单个事件发生的概率。Sigmoid适用于模型预测“是”或“否”的情形。</li>
<li><strong>多分类问题：</strong> 使用Softmax激活函数来处理输出层中的多个神经元。Softmax能将每个输出转化为概率分布，并保证所有类别概率之和等于1。</li>
</ul>
<h4 id="损失函数的区别"><a href="#损失函数的区别" class="headerlink" title="损失函数的区别"></a>损失函数的区别</h4><ul>
<li><strong>二分类问题：</strong> 在二分类场景下，常见损失函数是Binary Cross-Entropy Loss（BCE Loss），它量度了真实标签和模型预测之间差异程度。</li>
<li><strong>多分类问题：</strong> 多分类任务则通常采用Categorical Cross-Entropy Loss来评估预测准确性。当目标类别以整数形式表示时也可以使用Sparse Categorical Cross-Entropy Loss进行简化。</li>
</ul>
<p>总结来说，在CNN中解决不同类型的分类任务时，只有接近模型末端才会出现明显区分—主要体现在输出层设计、所选用激活函数以及损失函数定义上。其他方面，如卷积核大小、步长、填充等参数设置主要由输入数据特征及任务复杂性决定，并不直接受影响是否为二分类或者多分类的问题。</p>
<h4 id="卷积多分类的实现"><a href="#卷积多分类的实现" class="headerlink" title="卷积多分类的实现"></a>卷积多分类的实现</h4><p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E5%25A4%259A%25E5%2588%2586%25E7%25B1%25BB%25E6%25A0%2587%25E7%25AD%25BE.png" alt="多分类标签"></p>
<p>数据集来自<a target="_blank" rel="noopener" href="https://www.modelscope.cn/datasets/tany0699/flowers14/files">花朵分类 · 数据集 (modelscope.cn)</a></p>
<h5 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h5><p>首先，图片处理：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">shuzu=[]</span><br><span class="line">for dirpath, dirnames, files in os.walk(r&#x27;duofenl/val&#x27;):</span><br><span class="line">    for item in files:</span><br><span class="line">    if item.find(&quot;.jpg&quot;) != -1 or item.find(&quot;.JPG&quot;) != -1 or item.find(&quot;.png&quot;):</span><br><span class="line">             </span><br><span class="line">        picture = cv2.imread(dirpath+&quot;/&quot;+item) #趋向于0 0.0005332682630978525</span><br><span class="line">        resized_image = cv2.resize(picture, (224, 224))</span><br><span class="line">       </span><br><span class="line">        img_pil = Image.fromarray(cv2.cvtColor(resized_image,cv2.COLOR_BGR2RGB))</span><br><span class="line">        img_pil = toTensor(img_pil)</span><br><span class="line">        img_pil= transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ], std = [ 0.229, 0.224, 0.225 ](img_pil) </span><br><span class="line">               </span><br></pre></td></tr></table></figure>

<p>打标：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">if dirpath.find(r&quot;0&quot;) !=-1:</span><br><span class="line">              </span><br><span class="line">                    shuzu.append((img_pil,0))</span><br><span class="line">                if dirpath.find(r&quot;1&quot;) !=-1:</span><br><span class="line">          </span><br><span class="line">                    shuzu.append((img_pil,1))</span><br><span class="line">                if dirpath.find(r&quot;2&quot;) !=-1:</span><br><span class="line">     </span><br><span class="line">                    shuzu.append((img_pil,2))    </span><br><span class="line">                if dirpath.find(r&quot;3&quot;) !=-1:</span><br><span class="line">                 </span><br><span class="line">                    shuzu.append((img_pil,3)) </span><br></pre></td></tr></table></figure>



<p>调用resnet50模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载预训练的resnet模型</span></span><br><span class="line">model = resnet.resnet50()</span><br><span class="line">model_path =<span class="string">r&#x27;resnet50-19c8e357.pth&#x27;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(model_path):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;开始加载模型&#x27;</span>)</span><br><span class="line">    model.load_state_dict(torch.load(model_path))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;没有找到模型，退出！&#x27;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">model.fc=nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">2048</span>, <span class="number">4</span>)<span class="comment">#将原本的1000类改成4类</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>设置损失函数和优化器（切记损失函数的选择）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#criterion = nn.BCELoss() #二分类</span><br><span class="line">criterion =nn.CrossEntropyLoss() #多分类</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # 优化器</span><br></pre></td></tr></table></figure>

<p>训练模式开启，数组乱序（保证每次训练的数据不一样）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.train()</span><br><span class="line">random.shuffle(shuzu)</span><br></pre></td></tr></table></figure>

<p>为模型训练准备标签：</p>
<ol>
<li><p>初始化名为 <code>input</code> 和 <code>biaoqian</code> 的空列表，用于存放处理后的输入和标签。</p>
</li>
<li><p>遍历 <code>add</code> 列表，它可能包含了元组或列表形式的成对数据。其中，第一个元素 <code>i[0]</code> 是输入数据点，第二个元素 <code>i[1]</code> 是对应的类别标签。</p>
</li>
<li><p>将输入数据点添加到 <code>input</code> 列表中。</p>
</li>
<li><p>根据类别标签创建一个独热编码向量，并将其添加到biaoqian </p>
<p>列表中。例如：</p>
<ul>
<li>如果类别标签是 0，则生成向量 <code>[1,0,0,0]</code></li>
<li>如果类别标签是 1，则生成向量 <code>[0,1,0,0]</code></li>
<li>如果类别标签是 2，则生成向量 <code>[0,0,1,0]</code></li>
<li>如果类别标签是 3，则生成向量 <code>[0,0,0,1]</code></li>
</ul>
</li>
<li><p>然后使用PyTorch的 <code>torch.stack()</code> 方法沿着新维度堆叠输入张量列表并创建一个多维张量 <code>a</code>.</p>
</li>
<li><p>最后，将独热编码列表转换为PyTorch张量，并可能将其移动到指定设备上进行训练。</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">for i in range(32):#批次可以下降</span><br><span class="line">        </span><br><span class="line">        random_integer = random.randint(0, len(shuzu)-1)</span><br><span class="line">        add.append(shuzu[random_integer])</span><br><span class="line">    input=[]</span><br><span class="line">    biaoqian=[]</span><br><span class="line">    for i in add:</span><br><span class="line">        input.append(i[0])</span><br><span class="line">        if i[1]==0:</span><br><span class="line">            biaoqian.append([1,0,0,0])</span><br><span class="line">        if i[1]==1:</span><br><span class="line">            biaoqian.append([0,1,0,0])     </span><br><span class="line">        if i[1]==2:</span><br><span class="line">            biaoqian.append([0,0,1,0])     </span><br><span class="line">        if i[1]==3:</span><br><span class="line">            biaoqian.append([0,0,0,1])   </span><br><span class="line">    a = torch.stack(input, dim=0)</span><br><span class="line">    b = torch.tensor(biaoqian).to(device)</span><br></pre></td></tr></table></figure>

<p>开始训练：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">img=model(a.to(device))</span><br><span class="line">  </span><br><span class="line">  optimizer.zero_grad() #每次迭代都要把梯度置零</span><br><span class="line">  loss_contrastive= criterion(img, b.float())   # 计算两者的误差</span><br><span class="line">  loss_contrastive.backward()</span><br><span class="line">  optimizer.step()</span><br><span class="line">  if q %10==0:</span><br><span class="line">          print(loss_contrastive.item())#每十次查看一次</span><br><span class="line">          if loss_contrastive&lt;0.05:</span><br><span class="line">              torch.save(model.state_dict(), str(loss_contrastive.item())+&quot;.pkl&quot;)</span><br><span class="line">              print(&quot;保存权值结束程序！&quot;)</span><br></pre></td></tr></table></figure>

<p>源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#from model import SiameseNetwork</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> resnet</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dutupian1</span>():</span><br><span class="line">    shuzu=[]</span><br><span class="line">    <span class="keyword">for</span> dirpath, dirnames, files <span class="keyword">in</span> os.walk(<span class="string">r&#x27;duofenl/val&#x27;</span>):</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> files:</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> item.find(<span class="string">&quot;.jpg&quot;</span>) != -<span class="number">1</span> <span class="keyword">or</span> item.find(<span class="string">&quot;.JPG&quot;</span>) != -<span class="number">1</span> <span class="keyword">or</span> item.find(<span class="string">&quot;.png&quot;</span>):</span><br><span class="line"> </span><br><span class="line">                picture = cv2.imread(dirpath+<span class="string">&quot;/&quot;</span>+item) <span class="comment">#趋向于0 0.0005332682630978525</span></span><br><span class="line">                resized_image = cv2.resize(picture, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">       </span><br><span class="line">                img_pil = Image.fromarray(cv2.cvtColor(resized_image,cv2.COLOR_BGR2RGB))</span><br><span class="line">                img_pil = toTensor(img_pil)</span><br><span class="line">                img_pil= transforms.Normalize(mean = [ <span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span> ], std = [ <span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span> ])(img_pil) <span class="comment">#处理数据【-1，1】之间</span></span><br><span class="line">                <span class="keyword">if</span> dirpath.find(<span class="string">r&quot;0&quot;</span>) !=-<span class="number">1</span>:</span><br><span class="line">                  </span><br><span class="line">                    shuzu.append((img_pil,<span class="number">0</span>))</span><br><span class="line">                <span class="keyword">if</span> dirpath.find(<span class="string">r&quot;1&quot;</span>) !=-<span class="number">1</span>:</span><br><span class="line">              </span><br><span class="line">                    shuzu.append((img_pil,<span class="number">1</span>))</span><br><span class="line">                <span class="keyword">if</span> dirpath.find(<span class="string">r&quot;2&quot;</span>) !=-<span class="number">1</span>:</span><br><span class="line">                  </span><br><span class="line">                    shuzu.append((img_pil,<span class="number">2</span>))    </span><br><span class="line">                <span class="keyword">if</span> dirpath.find(<span class="string">r&quot;3&quot;</span>) !=-<span class="number">1</span>:</span><br><span class="line">                      </span><br><span class="line">                    shuzu.append((img_pil,<span class="number">3</span>)) </span><br><span class="line">                      </span><br><span class="line">    <span class="keyword">return</span> shuzu     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练的resnet模型</span></span><br><span class="line">model = resnet.resnet50()</span><br><span class="line">model_path =<span class="string">r&#x27;C:\Users\86184\PycharmProjects\pythonProject3\duofenl\resnet50-19c8e357.pth&#x27;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(model_path):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;开始加载模型&#x27;</span>)</span><br><span class="line">    model.load_state_dict(torch.load(model_path))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;没有找到模型，退出！&#x27;</span>)</span><br><span class="line">    exit()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line">model.fc=nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">2048</span>, <span class="number">4</span>)</span><br><span class="line">    <span class="comment">#nn.Sigmoid()</span></span><br><span class="line">)</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;当前设备是:&#x27;</span>,device)</span><br><span class="line">model.to(device)</span><br><span class="line">criterion =nn.CrossEntropyLoss() <span class="comment">#多分类</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.0001</span>) <span class="comment"># 优化器</span></span><br><span class="line"></span><br><span class="line">toTensor = transforms.ToTensor() </span><br><span class="line">shuzu=dutupian1()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>): <span class="comment">#训练测试</span></span><br><span class="line">    random.shuffle(shuzu)</span><br><span class="line">    model.train()</span><br><span class="line">    add=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">        </span><br><span class="line">        random_integer = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(shuzu)-<span class="number">1</span>)  <span class="comment"># 生成1到100之间的随机整数</span></span><br><span class="line">        add.append(shuzu[random_integer])</span><br><span class="line">    <span class="built_in">input</span>=[]</span><br><span class="line">    biaoqian=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> add:</span><br><span class="line">        <span class="built_in">input</span>.append(i[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> i[<span class="number">1</span>]==<span class="number">0</span>:</span><br><span class="line">            biaoqian.append([<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> i[<span class="number">1</span>]==<span class="number">1</span>:</span><br><span class="line">            biaoqian.append([<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>])     </span><br><span class="line">        <span class="keyword">if</span> i[<span class="number">1</span>]==<span class="number">2</span>:</span><br><span class="line">            biaoqian.append([<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>])     </span><br><span class="line">        <span class="keyword">if</span> i[<span class="number">1</span>]==<span class="number">3</span>:</span><br><span class="line">            biaoqian.append([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>])   </span><br><span class="line">    a = torch.stack(<span class="built_in">input</span>, dim=<span class="number">0</span>)</span><br><span class="line">    b = torch.tensor(biaoqian).to(device)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    img=model(a.to(device))</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad() <span class="comment">#每次迭代都要把梯度置零</span></span><br><span class="line">    loss_contrastive= criterion(img, b.<span class="built_in">float</span>())   <span class="comment"># 计算两者的误差</span></span><br><span class="line">    </span><br><span class="line">    loss_contrastive.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 优化参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> q %<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(loss_contrastive.item())</span><br><span class="line">            <span class="keyword">if</span> loss_contrastive&lt;<span class="number">0.05</span>:</span><br><span class="line">                torch.save(model.state_dict(), <span class="built_in">str</span>(loss_contrastive.item())+<span class="string">&quot;.pkl&quot;</span>)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;保存权值结束程序！&quot;</span>)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<h5 id="测试模型"><a href="#测试模型" class="headerlink" title="测试模型"></a>测试模型</h5><p>加载预训练模型并更改层</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = resnet.resnet50()</span><br><span class="line">model.fc=nn.Sequential(</span><br><span class="line">    nn.Linear(2048, 4)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>加载训练好的模型并检测模型是否存在：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model_path =r&#x27;0.003279013093560934.pkl&#x27;</span><br><span class="line">if os.path.exists(model_path):</span><br><span class="line">    print(&#x27;开始加载模型&#x27;)</span><br><span class="line">    model.load_state_dict(torch.load(model_path))</span><br><span class="line">else:</span><br><span class="line">    print(&#x27;没有找到模型，退出！&#x27;)</span><br><span class="line">    exit()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>接下来我们要进行以下步骤：</p>
<ol>
<li>初始化变量 <code>total</code> 和 <code>correct</code>，用于跟踪总共的样本数和正确的预测数。</li>
<li>调用 <code>dutupian1()</code> 函数，该函数可能是用来读取图片的。</li>
<li>进入一个循环，遍历变量 <code>shuzu</code> 中的每个元素。<code>shuzu</code> 可能是包含图片数据的列表。</li>
<li>在循环中，创建空列表 <code>imgs</code> 和 <code>add</code>，用于存储随机选择的图片和对应的标签。</li>
<li>使用 <code>random.randint()</code> 函数生成一个随机整数，作为索引从 <code>shuzu</code> 中选择图片和标签。</li>
<li>将选中的图片添加到 <code>imgs</code> 列表中，并将对应的标签添加到 <code>add</code> 列表中。</li>
<li>使用 <code>torch.stack()</code> 函数将 <code>imgs</code> 列表中的图片堆叠成一个张量 <code>a</code>，并指定 <code>dim=0</code> 表示在维度0上进行堆叠。</li>
<li>使用 <code>torch.tensor()</code> 函数将 <code>add</code> 列表转换为张量，并将结果存储在变量 <code>add</code> 中。</li>
<li>将张量 <code>a</code> 移动到设备 <code>device</code> 上，然后将其作为输入传递给模型 <code>model</code>，并将输出存储在变量 <code>outputs</code> 中。</li>
</ol>
<p>这段代码的目的可能是使用模型对随机选择的多个图片进行预测，并统计预测的准确性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">total = 0</span><br><span class="line">correct = 0</span><br><span class="line">dutupian1()</span><br><span class="line">for data in shuzu:</span><br><span class="line">    imgs = []</span><br><span class="line">    add = []</span><br><span class="line">    for i in range(6):</span><br><span class="line">        random_integer = random.randint(0, len(shuzu)-1)</span><br><span class="line">        imgs.append(shuzu[random_integer])</span><br><span class="line">        add.append(lab[random_integer])</span><br><span class="line">    a = torch.stack(imgs, dim=0)</span><br><span class="line">    add = torch.tensor(add)</span><br><span class="line">    outputs = model(a.to(device))</span><br></pre></td></tr></table></figure>

<p>接下来我们计算模型在测试集上的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用torch.max函数获取outputs张量中每行最大值的索引，并将结果赋值给predicted张量</span></span><br><span class="line">_, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将add张量的大小加到total变量上</span></span><br><span class="line">total += add.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将predicted张量与add张量转移到相同的设备上，并计算相等的元素个数，将结果累加到correct变量上</span></span><br><span class="line">correct += (predicted == add.to(device)).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>

<p>最后打印准确率：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&#x27;测试集中网络的准确率为: %d %%&#x27; % (100 * torch.true_divide(correct, total)))</span><br></pre></td></tr></table></figure>

<p>源码附上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">shuzu = []</span><br><span class="line">lab = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个函数用于读取图片</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dutupian1</span>():</span><br><span class="line">    <span class="keyword">for</span> dirpath, dirnames, files <span class="keyword">in</span> os.walk(<span class="string">r&#x27;duofenl/val&#x27;</span>):</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">if</span> item.find(<span class="string">&quot;.jpg&quot;</span>) != -<span class="number">1</span> <span class="keyword">or</span> item.find(<span class="string">&quot;.JPG&quot;</span>) != -<span class="number">1</span> <span class="keyword">or</span> item.find(<span class="string">&quot;.png&quot;</span>):</span><br><span class="line">                picture = cv2.imread(dirpath+<span class="string">&quot;/&quot;</span>+item)</span><br><span class="line">                resized_image = cv2.resize(picture, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">                img_pil = Image.fromarray(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))</span><br><span class="line">                img_pil = toTensor(img_pil)</span><br><span class="line">                img_pil = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])(img_pil)</span><br><span class="line">                <span class="keyword">if</span> dirpath.find(<span class="string">r&quot;0&quot;</span>) != -<span class="number">1</span>:</span><br><span class="line">                    shuzu.append(img_pil)</span><br><span class="line">                    lab.append(<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">if</span> dirpath.find(<span class="string">r&quot;1&quot;</span>) != -<span class="number">1</span>:</span><br><span class="line">                    shuzu.append(img_pil)</span><br><span class="line">                    lab.append(<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> dirpath.find(<span class="string">r&quot;2&quot;</span>) != -<span class="number">1</span>:</span><br><span class="line">                    shuzu.append(img_pil)</span><br><span class="line">                    lab.append(<span class="number">2</span>)</span><br><span class="line">                <span class="keyword">if</span> dirpath.find(<span class="string">r&quot;3&quot;</span>) != -<span class="number">1</span>:</span><br><span class="line">                    shuzu.append(img_pil)</span><br><span class="line">                    lab.append(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> shuzu</span><br><span class="line"></span><br><span class="line">toTensor = transforms.ToTensor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练的resnet模型</span></span><br><span class="line">model = resnet.resnet50()</span><br><span class="line">model.fc = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">2048</span>, <span class="number">4</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model_path = <span class="string">r&#x27;0.027752896770834923.pkl&#x27;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(model_path):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;开始加载模型&#x27;</span>)</span><br><span class="line">    model.load_state_dict(torch.load(model_path))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;没有找到模型，退出！&#x27;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;当前设备是:&#x27;</span>, device)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取图片并进行测试</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">dutupian1()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> shuzu:</span><br><span class="line">    imgs = []</span><br><span class="line">    add = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">        random_integer = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(shuzu)-<span class="number">1</span>)</span><br><span class="line">        imgs.append(shuzu[random_integer])</span><br><span class="line">        add.append(lab[random_integer])</span><br><span class="line">    a = torch.stack(imgs, dim=<span class="number">0</span>)</span><br><span class="line">    add = torch.tensor(add)</span><br><span class="line">    outputs = model(a.to(device))</span><br><span class="line">    _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">    total += add.size(<span class="number">0</span>)</span><br><span class="line">    correct += (predicted == add.to(device)).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集中网络的准确率为: %d %%&#x27;</span> % (<span class="number">100</span> * torch.true_divide(correct, total)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="孪生网络"><a href="#孪生网络" class="headerlink" title="孪生网络"></a>孪生网络</h2><h3 id="孪生网络简介"><a href="#孪生网络简介" class="headerlink" title="孪生网络简介"></a>孪生网络简介</h3><p>​	孪生网络（Siamese Network）是深度学习领域中用于比较两个输入数据项相似性的一种特殊类型的神经网络架构。孪生这个词来源于双胞胎，因为这种网络由两个完全相同的子网络组成，它们共享相同的参数和权重。</p>
<p>​	在训练过程中，孪生网络接收两个输入，并通过共享的子网络提取特征表示。这些特征表示被用来计算输入之间的相似度或距离。常见的相似度度量包括欧氏距离、余弦相似度等。</p>
<p>孪生网络常用于以下任务：</p>
<ol>
<li>人脸验证与识别：判断两张人脸图像是否属于同一人。</li>
<li>签名验证：判定两个签名是否来自同一人。</li>
<li>物体追踪：在视频序列中追踪特定物体。</li>
<li>相似商品推荐：找到与指定商品风格或属性最接近的其他商品。</li>
</ol>
<p>​	训练孪生网络时，通常使用“对比损失”（Contrastive Loss）或“三元组损失”（Triplet Loss）进行优化。这些损失函数鼓励网络使来自同一类别的示例更加接近，而不同类别的则更远分离。</p>
<p>​	孪生网络的训练是需要大量的标注数据，包括输入对的相似性标签。比较模型输出和标签之间的差异。</p>
<p>​	总而言之，孪生网络是一种用于比较两个输入之间相似性的深度学习模型，通过共享的子网络提取特征表示，并使用相似度度量来评估输入之间的相似性。</p>
<h3 id="孪生网络的损失函数"><a href="#孪生网络的损失函数" class="headerlink" title="孪生网络的损失函数"></a>孪生网络的损失函数</h3><p>孪生神经网络的损失函数通常用于衡量两个输入数据之间的相似性或差异性。常见的损失函数包括以下几种：</p>
<ol>
<li><p>对比损失（Contrastive Loss）：对比损失函数用于训练孪生网络，通过最小化相似样本对的距离，以及最大化不相似样本对的距离。常用的对比损失函数包括欧氏距离损失和三元组损失。</p>
</li>
<li><p>三元组损失（Triplet Loss）：三元组损失函数是对比损失函数的一种扩展形式，它通过构建三个样本的组合（锚定样本、正样本和负样本），来定义样本之间的相对关系。目标是使得锚定样本与正样本之间的距离小于锚定样本与负样本之间的距离。</p>
</li>
<li><p>余弦损失（Cosine Loss）：余弦损失函数用于衡量两个向量之间的余弦相似度。它通过最大化相似样本对的余弦相似度，以及最小化不相似样本对的余弦相似度来训练孪生网络。</p>
</li>
<li><p>交叉熵损失（Cross-Entropy Loss）：交叉熵损失函数常用于分类任务，可以用于训练孪生网络进行相似性分类。通过将相似性标签转化为概率分布，并与模型输出的概率分布进行比较，来计算损失值。</p>
</li>
</ol>
<p>这些损失函数的选择取决于具体的任务和数据特点。在实际应用中，根据具体情况选择适合的损失函数来训练孪生网络，以达到最佳的相似性比较效果。</p>
<h3 id="动手实现一个孪生网络"><a href="#动手实现一个孪生网络" class="headerlink" title="动手实现一个孪生网络"></a>动手实现一个孪生网络</h3><h4 id="行人重识别"><a href="#行人重识别" class="headerlink" title="行人重识别"></a>行人重识别</h4><p>孪生网络在监控系统中广泛应用于行人重识别任务。通过比对两个不同时间和地点捕捉到的行人图像，网络能够快速准确地辨认出是否为同一行人，为安防领域提供了强大的技术支持。</p>
<p>例如，在一个大型购物中心的监控系统中，两个相邻摄像头捕捉到了同一位顾客的图像。孪生网络接收这两张图像，通过学习提取出顾客的特征，如身高、服装等。通过对比学习，网络能够判断这两张图像中是否是同一个顾客，为购物中心的安全管理提供了高效的行人重识别技术。</p>
<p>接下来我们来动手实现该网络。</p>
<h4 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h4><p>我们继续使用resnet50网络，但是我们需要更改一下他的输出层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_forward_impl</span>(<span class="params">self, x: Tensor, y: Tensor</span>) -&gt; Tensor:</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># See note [TorchScript super()]</span></span><br><span class="line">    x = self.conv1(x)</span><br><span class="line">    x = self.bn1(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.maxpool(x)</span><br><span class="line">    x = self.layer1(x)</span><br><span class="line"></span><br><span class="line">    x = self.layer2(x)</span><br><span class="line">    x = self.layer3(x)</span><br><span class="line">    x = self.layer4(x)</span><br><span class="line">    x = self.avgpool(x)</span><br><span class="line">    x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    y = self.conv1(y)</span><br><span class="line">    y = self.bn1(y)</span><br><span class="line">    y = self.relu(y)</span><br><span class="line">    y = self.maxpool(y)</span><br><span class="line">    y = self.layer1(y)</span><br><span class="line">    y = self.layer2(y)</span><br><span class="line">    y = self.layer3(y)</span><br><span class="line">    y = self.layer4(y)</span><br><span class="line">    y = self.avgpool(y)</span><br><span class="line">    y = torch.flatten(y, <span class="number">1</span>)</span><br><span class="line">    output=x-y</span><br><span class="line">    output = self.fc(output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> 合并</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor,y: Tensor</span>) -&gt; Tensor:</span><br><span class="line">    <span class="keyword">return</span> self._forward_impl(x,y)</span><br></pre></td></tr></table></figure>

<p>把网络改完之后，我们还是对数据集进行手动打标，孪生网络通常用于比较两个输入之间的相似性或距离。在孪生网络中，数据集的要求与其他深度学习模型有一些不同。</p>
<h4 id="数据集处理"><a href="#数据集处理" class="headerlink" title="数据集处理"></a>数据集处理</h4><p>要求如下：</p>
<ol>
<li><p>数据对：数据集应包含成对的样本，每个样本对应一个标签。</p>
</li>
<li><p>样本标签：每个样本对应一个标签，用于表示样本对的相似性或不相似性。通常，正样本对的标签为1，负样本对的标签为0。</p>
</li>
</ol>
<p>定义一个函数<code>dutupian()</code>，用于创建数据集。该函数首先遍历指定路径下的文件夹，然后随机选择两张图片作为正样本对，并进行预处理操作，包括图像裁剪、颜色变换和数据标准化等。同时，还会随机选择一张不同类别的图片作为负样本，并进行相同的预处理操作。最后，将处理后的图片和对应的标签添加到<code>img0</code>、<code>img1</code>和<code>lab</code>列表中。</p>
<p>第一组数据对：</p>
<p>（以下是一个人两天从同一个摄像头下经过，设其标签seed为1）</p>
<p>![LSWL 0 (C:&#x2F;Users&#x2F;86150&#x2F;Documents&#x2F;WeChat%2520Files&#x2F;wxid_1tzda7negup922&#x2F;FileStorage&#x2F;File&#x2F;2024-01&#x2F;LSWL%25200%2520(1).png)](LSWL 0 (1).png)</p>
<p>![LSWL 0 (C:&#x2F;Users&#x2F;86150&#x2F;Documents&#x2F;WeChat%2520Files&#x2F;wxid_1tzda7negup922&#x2F;FileStorage&#x2F;File&#x2F;2024-01&#x2F;LSWL%25200%2520(2).png)](LSWL 0 (2).png)</p>
<p>第二组数据对：</p>
<p>（以下是不同的两人两天从同一个摄像头下经过，设其标签seed为0）</p>
<p>![LSWL1 (C:&#x2F;Users&#x2F;86150&#x2F;Documents&#x2F;WeChat%2520Files&#x2F;wxid_1tzda7negup922&#x2F;FileStorage&#x2F;File&#x2F;2024-01&#x2F;LSWL1%2520(1).png)](LSWL1 (1).png)</p>
<p>![LSWL1 (C:&#x2F;Users&#x2F;86150&#x2F;Documents&#x2F;WeChat%2520Files&#x2F;wxid_1tzda7negup922&#x2F;FileStorage&#x2F;File&#x2F;2024-01&#x2F;LSWL1%2520(2).png)](LSWL1 (2).png)</p>
<p>思路如下：</p>
<ol>
<li><p>遍历指定路径下的文件夹，将文件夹路径添加到<code>suiji</code>列表中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> dirpath, dirnames, files <span class="keyword">in</span> os.walk(<span class="string">r&#x27;datalist\train&#x27;</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.samefile(dirpath, <span class="string">&#x27;datalist\\train&#x27;</span>):</span><br><span class="line">            suiji.append(dirpath)</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于每个文件夹，首先生成一个随机数<code>seed</code>，用于确定是选择正样本对还是负样本对。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> dirpath, dirnames, files <span class="keyword">in</span> os.walk(<span class="string">r&#x27;datalist\train&#x27;</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.samefile(dirpath, <span class="string">&#x27;datalist\\train&#x27;</span>):</span><br><span class="line">            seed = random.randint(<span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>


</li>
<li><p>如果<code>seed</code>为1，表示选择正样本对。从当前文件夹中随机选择两张图片，并进行预处理操作，包括图像裁剪、颜色变换和数据标准化等。将处理后的图片添加到<code>img0</code>和<code>img1</code>列表中，并将对应的标签1添加到<code>lab</code>列表中。</p>
</li>
<li><p>如果<code>seed</code>为0，表示选择负样本对。从当前文件夹中随机选择一张图片，并进行预处理操作。然后从<code>suiji</code>列表中随机选择一个不同类别的文件夹，再从该文件夹中随机选择一张图片，进行相同的预处理操作。将处理后的图片添加到<code>img0</code>和<code>img1</code>列表中，并将对应的标签0添加到<code>lab</code>列表中。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> seed == <span class="number">1</span>:</span><br><span class="line">             bs = os.listdir(dirpath)</span><br><span class="line">             random.shuffle(bs)</span><br><span class="line">             a = cv2.imread(dirpath+<span class="string">&#x27;\\&#x27;</span>+bs[<span class="number">0</span>])</span><br><span class="line">             resized_image = cv2.resize(a, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">             img_pil = Image.fromarray(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))</span><br><span class="line">             img_pil = toTensor(img_pil)</span><br><span class="line">             img_pil = transforms.RandomHorizontalFlip()(img_pil)  <span class="comment"># 50%的概率进行随机水平翻转</span></span><br><span class="line">             img_pil = transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0</span>)(img_pil)</span><br><span class="line">             img_pil = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])(</span><br><span class="line">                 img_pil)  <span class="comment"># 处理数据【-1，1】之间</span></span><br><span class="line">             img0.append(img_pil)</span><br><span class="line">             </span><br><span class="line">             b=cv2.imread(dirpath+<span class="string">&quot;\\&quot;</span>+bs[<span class="number">1</span>])</span><br><span class="line">             resized_image = cv2.resize(b, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">             img_pil = Image.fromarray(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))</span><br><span class="line">             img_pil = toTensor(img_pil)</span><br><span class="line">             img_pil = transforms.RandomHorizontalFlip()(img_pil)  <span class="comment"># 50%的概率进行随机水平翻转</span></span><br><span class="line">             img_pil = transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0</span>)(img_pil)</span><br><span class="line">             img_pil = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])(</span><br><span class="line">                 img_pil)  <span class="comment"># 处理数据【-1，1】之间</span></span><br><span class="line"></span><br><span class="line">             img1.append(img_pil)</span><br><span class="line"></span><br><span class="line">             lab.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">         <span class="keyword">if</span> seed == <span class="number">0</span>:</span><br><span class="line">             bs = os.listdir(dirpath)</span><br><span class="line">             random.shuffle(bs)</span><br><span class="line">             a = cv2.imread(dirpath+<span class="string">&#x27;\\&#x27;</span>+bs[<span class="number">0</span>])</span><br><span class="line">             resized_image = cv2.resize(a, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">             img_pil = Image.fromarray(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))</span><br><span class="line">             img_pil = toTensor(img_pil)</span><br><span class="line">             img_pil = transforms.RandomHorizontalFlip()(img_pil)  <span class="comment"># 50%的概率进行随机水平翻转</span></span><br><span class="line">             img_pil = transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0</span>)(img_pil)</span><br><span class="line">             img_pil = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])(</span><br><span class="line">                 img_pil)  <span class="comment"># 处理数据【-1，1】之间</span></span><br><span class="line">             img0.append(img_pil)</span><br><span class="line"></span><br><span class="line">             <span class="keyword">while</span> (<span class="number">1</span>):</span><br><span class="line">                 random.shuffle(suiji)</span><br><span class="line">                 suiji[<span class="number">0</span>] !=bs[<span class="number">0</span>]</span><br><span class="line">                 <span class="keyword">break</span></span><br><span class="line">             </span><br><span class="line">             <span class="keyword">for</span> path <span class="keyword">in</span> suiji:</span><br><span class="line">                 bs = os.listdir(path)</span><br><span class="line">                 random.shuffle(bs)</span><br><span class="line">                 a = cv2.imread(path + <span class="string">&#x27;\\&#x27;</span> + bs[<span class="number">0</span>])</span><br><span class="line">                 resized_image = cv2.resize(a, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">                 img_pil = Image.fromarray(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))</span><br><span class="line">                 img_pil = toTensor(img_pil)</span><br><span class="line">                 img_pil = transforms.RandomHorizontalFlip()(img_pil)  <span class="comment"># 50%的概率进行随机水平翻转</span></span><br><span class="line">                 img_pil = transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0</span>)(img_pil)</span><br><span class="line">                 img_pil = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])(</span><br><span class="line">                     img_pil)  <span class="comment"># 处理数据【-1，1】之间</span></span><br><span class="line"></span><br><span class="line">                 img1.append(img_pil)</span><br><span class="line"></span><br><span class="line">             lab.append(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>首先，通过<code>resnet50(False)</code>创建了一个ResNet-50的实例，并将其赋值给变量<code>model</code>。<code>False</code>表示不加载预训练的权重，我们将在训练过程中从头开始训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model=resnet50网络<span class="number">1.</span>resnet50(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>接下来，使用<code>transforms.ToTensor()</code>创建了一个转换器<code>toTensor</code>，用于将图像数据转换为张量格式。最好定义在上面，因为图片那里用了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">toTensor = transforms.ToTensor()</span><br></pre></td></tr></table></figure>

<p>然后，通过<code>torch.device</code>判断当前设备是使用GPU还是CPU，并将其赋值给变量<code>device</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line">print(&#x27;当前设备是:&#x27;,device)</span><br></pre></td></tr></table></figure>

<p>接着，使用<code>model.to(device)</code>将模型移动到对应的设备上，以便后续的训练和推理操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>

<p>然后，使用<code>nn.BCELoss()</code>创建了一个二分类的损失函数<code>criterion</code>，即二分类交叉熵损失函数。	</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.BCELoss() #二分类</span><br></pre></td></tr></table></figure>

<p>最后，使用<code>torch.optim.Adam</code>创建了一个Adam优化器<code>optim</code>，并将模型的参数传递给优化器进行优化。学习率(lr)被设置为0.0001，可以根据需要进行调整。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optim = torch.optim.Adam(model.parameters(),lr=0.0001)</span><br></pre></td></tr></table></figure>

<p>接下来的<code>epoch=500</code>表示训练的轮数，即将训练数据集迭代500次。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">epoch=500</span><br></pre></td></tr></table></figure>

<p>下面这段代码是在每个epoch中进行训练的循环。</p>
<p>首先，创建了三个空列表<code>img2</code>、<code>img3</code>和<code>lab1</code>，用于存储样本对和对应的标签。</p>
<p>接着，通过<code>flag=len(img0)-1</code>获取<code>img0</code>列表的长度，并将其减去1赋值给<code>flag</code>变量。</p>
<p>然后，使用一个循环迭代32次，每次迭代从<code>img0</code>、<code>img1</code>和<code>lab</code>列表中取出对应索引为<code>flag</code>的元素，并分别添加到<code>img2</code>、<code>img3</code>和<code>lab1</code>列表中。然后，将<code>flag</code>减1，以便在下一次迭代中获取下一个样本对和标签。如果<code>flag</code>小于0，则跳出循环。</p>
<p>接下来，调用<code>model.train()</code>将模型设置为训练模式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    img2,img3,lab1=[],[],[]</span><br><span class="line">    flag=<span class="built_in">len</span>(img0)-<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>):</span><br><span class="line">        img2.append(img0[flag])</span><br><span class="line">        img3.append(img1[flag])</span><br><span class="line">        lab1.append(lab[flag])</span><br><span class="line">        flag -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> flag&lt;<span class="number">0</span> :</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    model.train()<span class="comment">#开启训练模式</span></span><br></pre></td></tr></table></figure>

<p>开启训练模式后，</p>
<ol>
<li>首先，通过<code>torch.stack</code>函数将<code>img2</code>和<code>img3</code>两个图像列表按照指定的维度（<code>dim=0</code>）进行堆叠，生成张量<code>a</code>和<code>b</code>。这里假设<code>img2</code>和<code>img3</code>是经过预处理后的图像数据。</li>
<li>接下来，将<code>lab1</code>转换为张量，并使用<code>unsqueeze(1)</code>函数在第一维度上增加一个维度。这是因为<code>lab1</code>是标签数据，需要与模型输出的结果进行比较，所以需要与模型输出的形状保持一致。</li>
<li>使用<code>Variable</code>函数将<code>a</code>、<code>b</code>和<code>lab1</code>转换为可求导的变量，并将它们移动到GPU上（假设有可用的GPU）。这是为了在训练过程中能够计算梯度并使用GPU加速计算。</li>
<li>调用模型的<code>__call__</code>方法，将<code>a</code>和<code>b</code>作为输入，得到模型的输出<code>组合</code>。这里的<code>model</code>是一个Siamese网络模型的实例，<code>__call__</code>方法会调用模型的前向传播方法。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.stack(img2, dim=0)</span><br><span class="line">   b = torch.stack(img3, dim=0)</span><br><span class="line">   lab1=torch.tensor(lab1)</span><br><span class="line">   lab1 = lab1.unsqueeze(1)</span><br><span class="line">   a, b,lab1= Variable(a).cuda(), Variable(b).cuda(), Variable(lab1).cuda()</span><br><span class="line">   组合=model(a,b)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li>通过<code>optim.zero_grad()</code>将模型参数的梯度清零，准备进行反向传播计算梯度。</li>
<li>使用<code>criterion</code>（可能是<code>torch.nn.BCELoss</code>的实例）计算模型输出<code>组合</code>与标签<code>lab1</code>之间的损失值<code>loss_contrastive</code>。这里假设是使用二分类交叉熵损失函数，将<code>组合</code>和<code>lab1</code>转换为浮点型。</li>
<li>调用<code>loss_contrastive.backward(retain_graph=True)</code>进行反向传播计算梯度，并保留计算图。这样可以在后续的优化步骤中继续使用计算图进行计算。</li>
<li>调用<code>optim.step()</code>来更新模型的参数，根据梯度和优化算法（可能是<code>torch.optim.Adam</code>的实例）来更新模型参数。</li>
<li>将<code>loss_contrastive.item()</code>赋值给<code>val_loss</code>，将损失值转换为Python标量。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">optim.zero_grad()</span><br><span class="line">loss_contrastive= criterion(组合, lab1.float())   # 计算两者的误差</span><br><span class="line"></span><br><span class="line">loss_contrastive.backward(retain_graph=True)</span><br><span class="line"></span><br><span class="line"># 优化参数</span><br><span class="line">optim.step()</span><br><span class="line">val_loss=loss_contrastive.item()</span><br></pre></td></tr></table></figure>

<p>检查损失函数（loss_contrastive）是否小于等于0.05。如果是，则将模型的状态字典（model.state_dict()）保存到一个.pth文件中，并打印出”模型已经保存”的消息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if loss_contrastive &lt;= 0.05:</span><br><span class="line">        torch.save(model.state_dict(),&quot;./&#123;&#125;.pth&quot;.format(loss_contrastive))</span><br><span class="line">        print(&quot;模型已经保存&quot;)</span><br></pre></td></tr></table></figure>

<p>现在，我们已经成功地完成孪生网络的训练阶段。</p>
<p>源码附上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#from model import SiameseNetwork</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> resnet50网络<span class="number">1</span></span><br><span class="line"></span><br><span class="line">img0, img1,lab = [], [],[]</span><br><span class="line">toTensor = transforms.ToTensor()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dutupian</span>():</span><br><span class="line">    </span><br><span class="line">    suiji = []</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> dirpath, dirnames, files <span class="keyword">in</span> os.walk(<span class="string">r&#x27;datalist\train&#x27;</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.samefile(dirpath, <span class="string">&#x27;datalist\\train&#x27;</span>):</span><br><span class="line">            suiji.append(dirpath)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> dirpath, dirnames, files <span class="keyword">in</span> os.walk(<span class="string">r&#x27;datalist\train&#x27;</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.samefile(dirpath, <span class="string">&#x27;datalist\\train&#x27;</span>):</span><br><span class="line">            seed = random.randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            <span class="built_in">print</span>(seed)</span><br><span class="line">            <span class="keyword">if</span> seed == <span class="number">1</span>:</span><br><span class="line">                bs = os.listdir(dirpath)</span><br><span class="line">                random.shuffle(bs)</span><br><span class="line">                a = cv2.imread(dirpath+<span class="string">&#x27;\\&#x27;</span>+bs[<span class="number">0</span>])</span><br><span class="line">                resized_image = cv2.resize(a, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">                img_pil = Image.fromarray(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))</span><br><span class="line">                img_pil = toTensor(img_pil)</span><br><span class="line">                img_pil = transforms.RandomHorizontalFlip()(img_pil)  <span class="comment"># 50%的概率进行随机水平翻转</span></span><br><span class="line">                img_pil = transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0</span>)(img_pil)</span><br><span class="line">                img_pil = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])(</span><br><span class="line">                    img_pil)  <span class="comment"># 处理数据【-1，1】之间</span></span><br><span class="line">                img0.append(img_pil)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                <span class="comment"># bs = os.listdir(dirpath)</span></span><br><span class="line">                <span class="comment"># random.shuffle(bs)</span></span><br><span class="line">                <span class="comment"># b = cv2.imread(dirpath+&#x27;\\&#x27;+bs[0])</span></span><br><span class="line">                b=cv2.imread(dirpath+<span class="string">&quot;\\&quot;</span>+bs[<span class="number">1</span>])</span><br><span class="line">                resized_image = cv2.resize(b, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">                img_pil = Image.fromarray(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))</span><br><span class="line">                img_pil = toTensor(img_pil)</span><br><span class="line">                img_pil = transforms.RandomHorizontalFlip()(img_pil)  <span class="comment"># 50%的概率进行随机水平翻转</span></span><br><span class="line">                img_pil = transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0</span>)(img_pil)</span><br><span class="line">                img_pil = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])(</span><br><span class="line">                    img_pil)  <span class="comment"># 处理数据【-1，1】之间</span></span><br><span class="line"></span><br><span class="line">                img1.append(img_pil)</span><br><span class="line"></span><br><span class="line">                lab.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> seed == <span class="number">0</span>:</span><br><span class="line">                bs = os.listdir(dirpath)</span><br><span class="line">                random.shuffle(bs)</span><br><span class="line">                a = cv2.imread(dirpath+<span class="string">&#x27;\\&#x27;</span>+bs[<span class="number">0</span>])</span><br><span class="line">                resized_image = cv2.resize(a, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">                img_pil = Image.fromarray(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))</span><br><span class="line">                img_pil = toTensor(img_pil)</span><br><span class="line">                img_pil = transforms.RandomHorizontalFlip()(img_pil)  <span class="comment"># 50%的概率进行随机水平翻转</span></span><br><span class="line">                img_pil = transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0</span>)(img_pil)</span><br><span class="line">                img_pil = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])(</span><br><span class="line">                    img_pil)  <span class="comment"># 处理数据【-1，1】之间</span></span><br><span class="line">                img0.append(img_pil)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">while</span> (<span class="number">1</span>):</span><br><span class="line">                    random.shuffle(suiji)</span><br><span class="line">                    suiji[<span class="number">0</span>] !=bs[<span class="number">0</span>]</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">for</span> path <span class="keyword">in</span> suiji:</span><br><span class="line">                    bs = os.listdir(path)</span><br><span class="line">                    random.shuffle(bs)</span><br><span class="line">                    a = cv2.imread(path + <span class="string">&#x27;\\&#x27;</span> + bs[<span class="number">0</span>])</span><br><span class="line">                    resized_image = cv2.resize(a, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">                    img_pil = Image.fromarray(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))</span><br><span class="line">                    img_pil = toTensor(img_pil)</span><br><span class="line">                    img_pil = transforms.RandomHorizontalFlip()(img_pil)  <span class="comment"># 50%的概率进行随机水平翻转</span></span><br><span class="line">                    img_pil = transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0</span>)(img_pil)</span><br><span class="line">                    img_pil = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])(</span><br><span class="line">                        img_pil)  <span class="comment"># 处理数据【-1，1】之间</span></span><br><span class="line"></span><br><span class="line">                    img1.append(img_pil)</span><br><span class="line"></span><br><span class="line">                lab.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">                </span><br><span class="line"></span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dutupian()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model=resnet50网络<span class="number">1.</span>resnet50(<span class="literal">False</span>)</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;当前设备是:&#x27;</span>,device)</span><br><span class="line">model.to(device)</span><br><span class="line">criterion = nn.BCELoss() <span class="comment">#二分类x</span></span><br><span class="line"> <span class="comment"># 优化器</span></span><br><span class="line">optim = torch.optim.Adam(model.parameters(),lr=<span class="number">0.0001</span>)</span><br><span class="line"></span><br><span class="line">epoch=<span class="number">500</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    img2,img3,lab1=[],[],[]</span><br><span class="line">    flag=<span class="built_in">len</span>(img0)-<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        img2.append(img0[flag])</span><br><span class="line">        img3.append(img1[flag])</span><br><span class="line">        lab1.append(lab[flag])</span><br><span class="line">        flag -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> flag&lt;<span class="number">0</span> :</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    model.train()<span class="comment">#开启训练模式</span></span><br><span class="line"></span><br><span class="line">    a = torch.stack(img2, dim=<span class="number">0</span>)</span><br><span class="line">    b = torch.stack(img3, dim=<span class="number">0</span>)</span><br><span class="line">    lab1=torch.tensor(lab1)</span><br><span class="line">    lab1 = lab1.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    a, b,lab1= Variable(a).cuda(), Variable(b).cuda(), Variable(lab1).cuda()</span><br><span class="line">    组合=model(a,b)</span><br><span class="line"></span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    loss_contrastive= criterion(组合, lab1.<span class="built_in">float</span>())   <span class="comment"># 计算两者的误差</span></span><br><span class="line">    </span><br><span class="line">    loss_contrastive.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优化参数</span></span><br><span class="line">    optim.step()</span><br><span class="line">    val_loss=loss_contrastive.item()</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(i)</span><br><span class="line">        <span class="built_in">print</span>(loss_contrastive.item())</span><br><span class="line">    <span class="keyword">if</span> loss_contrastive &lt;= <span class="number">0.0001</span>:</span><br><span class="line">        torch.save(model.state_dict(),<span class="string">&quot;./&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(loss_contrastive))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;模型已经保存&quot;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="孪生网络测试"><a href="#孪生网络测试" class="headerlink" title="孪生网络测试"></a>孪生网络测试</h3><p>我们将要对上一节完成的孪生网络模型进行测试。</p>
<ol>
<li>加载测试数据集文件夹列表，并对其进行随机打乱：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">所有的文件夹 = os.listdir(<span class="string">r&quot;LSWL\datalist\test&quot;</span>)</span><br><span class="line">random.shuffle(所有的文件夹)</span><br></pre></td></tr></table></figure>

<p>这里使用os模块的listdir()函数获取测试数据集文件夹列表，并使用random模块的shuffle()函数对列表进行随机打乱。</p>
<p>​	2.定义空列表用于存储图像数据和标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img0, img1 = [], []</span><br><span class="line">lab = []</span><br></pre></td></tr></table></figure>

<p>这里定义了两个空列表img0和img1，分别用于存储样本对中的第一张图像和第二张图像；还定义了一个空列表lab，用于存储样本对的标签。</p>
<p>​	3.创建数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    random_integer = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(所有的文件夹)-<span class="number">1</span>)</span><br><span class="line">    逻辑 = random.randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    文件夹名字 = 所有的文件夹[random_integer]</span><br></pre></td></tr></table></figure>

<p>这里使用for循环创建数据集。首先使用random模块的randint()函数生成一个随机整数random_integer，作为文件夹列表的索引。然后使用random模块的randint()函数生成一个逻辑值逻辑，用于选择正样本（逻辑值为1）或负样本（逻辑值为0）。最后根据随机整数获取对应的文件夹名字。</p>
<p>​	4.根据逻辑值选择正样本或负样本，并加载图像数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> 逻辑 == <span class="number">1</span>:</span><br><span class="line">    bs = os.listdir(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span> + 文件夹名字)</span><br><span class="line">    random.shuffle(bs)</span><br><span class="line">    a = cv2.imread(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span>+文件夹名字+<span class="string">&quot;/&quot;</span>+bs[<span class="number">0</span>])</span><br><span class="line">    resized_image = cv2.resize(a, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    img0.append(resized_image)</span><br><span class="line">    b = cv2.imread(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span>+文件夹名字+<span class="string">&quot;/&quot;</span>+bs[<span class="number">1</span>])</span><br><span class="line">    resized_image = cv2.resize(b, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    img1.append(resized_image)</span><br><span class="line">    lab.append(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>如果逻辑值为1，表示选择正样本。首先使用os模块的listdir()函数获取指定文件夹中的文件列表bs，并使用random模块的shuffle()函数对列表进行随机打乱。然后使用cv2库的imread()函数加载第一张图像，并使用cv2库的resize()函数将图像大小调整为224x224。接着将调整大小后的图像添加到img0列表中。同样地，加载第二张图像并添加到img1列表中。最后将标签1添加到lab列表中，表示这是一对正样本。</p>
<p>​	5.如果逻辑值为0，则加载负样本图像：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> 逻辑 == <span class="number">0</span>:</span><br><span class="line">    bs = os.listdir(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span>+文件夹名字)</span><br><span class="line">    random.shuffle(bs)</span><br><span class="line">    a = cv2.imread(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span>+文件夹名字+<span class="string">&quot;/&quot;</span>+bs[<span class="number">0</span>])</span><br><span class="line">    resized_image = cv2.resize(a, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    img0.append(resized_image)</span><br><span class="line">    </span><br><span class="line">    suiji = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(所有的文件夹)-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">while</span> suiji == random_integer:</span><br><span class="line">        suiji = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(所有的文件夹)-<span class="number">1</span>)</span><br><span class="line">    bs = os.listdir(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span> + 所有的文件夹[suiji])</span><br><span class="line">    random.shuffle(bs)</span><br><span class="line">    a = cv2.imread(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span> + 所有的文件夹[suiji]+<span class="string">&quot;/&quot;</span>+bs[<span class="number">0</span>])</span><br><span class="line">    resized_image = cv2.resize(a, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">    img1.append(resized_image)</span><br><span class="line">    lab.append(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>如果逻辑值为0，表示选择负样本。跟逻辑值为1的操作基本相同最后将调整大小后的图像添加到img1列表中，并将标签0添加到lab列表中，表示这是一对负样本。</p>
<p>​	6.加载预训练的ResNet-50模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = resnet50网络<span class="number">1.</span>resnet50(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>这里使用自定义的resnet50网络模型中的resnet50()函数创建一个ResNet-50模型对象model。参数False表示不加载预训练的权重。</p>
<p>​	7.将模型移动到GPU（如果可用）或CPU上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>

<p>这里使用torch.cuda.is_available()函数判断是否可用GPU，如果可用则将模型移动到GPU上，否则移动到CPU上。</p>
<p>​	8.加载已保存的模型参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model_path = <span class="string">r&#x27;1.0483641744940542e-05.pth&#x27;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(model_path):</span><br><span class="line">    model.load_state_dict(torch.load(model_path))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;没有找到模型，退出！&#x27;</span>)</span><br><span class="line">    exit()</span><br></pre></td></tr></table></figure>

<p>​	9.设置模型为测试模式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>

<p>​	10.对每个样本对进行预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):<span class="comment">#可以改，进行二十轮</span></span><br><span class="line">    img2 = []</span><br><span class="line">    img3 = []</span><br><span class="line">    lab1 = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):</span><br><span class="line">        <span class="comment">#由于在进行图片处理时没有对图片进行处理，所以在这里进行处理</span></span><br><span class="line">        img_pil = Image.fromarray(cv2.cvtColor(img0[j], cv2.COLOR_BGR2RGB))</span><br><span class="line">        img_pil = toTensor(img_pil)</span><br><span class="line">        img_pil = transforms.RandomHorizontalFlip()(img_pil)</span><br><span class="line">        img_pil = transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0</span>)(img_pil)</span><br><span class="line">        img_pil = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])(img_pil)</span><br><span class="line">        img2.append(img_pil)</span><br><span class="line">    </span><br><span class="line">        img_pil = Image.fromarray(cv2.cvtColor(img1[j], cv2.COLOR_BGR2RGB))</span><br><span class="line">        img_pil = toTensor(img_pil)</span><br><span class="line">        img_pil = transforms.RandomHorizontalFlip()(img_pil)</span><br><span class="line">        img_pil = transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0</span>)(img_pil)</span><br><span class="line">        img_pil = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])(img_pil)</span><br><span class="line">        img3.append(img_pil)</span><br><span class="line">    	lab.append(lab[j])</span><br><span class="line">    a = torch.stack(img2, dim=<span class="number">0</span>)</span><br><span class="line">    b = torch.stack(img3, dim=<span class="number">0</span>)</span><br><span class="line">    lab = torch.tensor(lab)</span><br><span class="line">    a, b, lab = Variable(a).to(device), Variable(b).to(device), Variable(lab).to(device)</span><br><span class="line">    outputs = model(a, b)</span><br><span class="line">    array = outputs.cpu().detach().numpy()</span><br><span class="line">    array = (array &gt; <span class="number">0.5</span>)</span><br><span class="line">    ants_values = array[<span class="number">0</span>]</span><br><span class="line">    ants_valuess = ants_values[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(ants_valuess)</span><br></pre></td></tr></table></figure>

<p>源码附上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> resnet50网络<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">toTensor = transforms.ToTensor() </span><br><span class="line"></span><br><span class="line">所有的文件夹=os.listdir(<span class="string">r&quot;LSWL\datalist\test&quot;</span>)</span><br><span class="line">random.shuffle(所有的文件夹)</span><br><span class="line"></span><br><span class="line">img0,img1=[],[]</span><br><span class="line">lab=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    random_integer = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(所有的文件夹)-<span class="number">1</span>)  <span class="comment"># 生成所有文件夹总数的随机数</span></span><br><span class="line">    <span class="comment">#print(random_integer)</span></span><br><span class="line">    逻辑 = random.randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    文件夹名字=所有的文件夹[random_integer]</span><br><span class="line">    <span class="built_in">print</span>(逻辑)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> 逻辑==<span class="number">1</span>:</span><br><span class="line">        bs = os.listdir(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span> + 文件夹名字)</span><br><span class="line"></span><br><span class="line">        random.shuffle(bs)</span><br><span class="line">        a=cv2.imread(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span>+文件夹名字+<span class="string">&quot;/&quot;</span>+bs[<span class="number">0</span>])</span><br><span class="line">        resized_image = cv2.resize(a, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">        img0.append(resized_image)</span><br><span class="line">        </span><br><span class="line">        b=cv2.imread(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span>+文件夹名字+<span class="string">&quot;/&quot;</span>+bs[<span class="number">1</span>])</span><br><span class="line">        resized_image = cv2.resize(b, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">        img1.append(resized_image)</span><br><span class="line">        </span><br><span class="line">        lab.append(<span class="number">1</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span>+文件夹名字+<span class="string">&quot;/&quot;</span>+bs[<span class="number">1</span>])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span>+文件夹名字+<span class="string">&quot;/&quot;</span>+bs[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> 逻辑==<span class="number">0</span>:</span><br><span class="line">        <span class="comment">#print(文件夹名字)</span></span><br><span class="line">        bs=os.listdir(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span>+文件夹名字)</span><br><span class="line">        random.shuffle(bs)</span><br><span class="line">        a=cv2.imread(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span>+文件夹名字+<span class="string">&quot;/&quot;</span>+bs[<span class="number">0</span>])</span><br><span class="line">        resized_image = cv2.resize(a, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">        img0.append(resized_image)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span>+文件夹名字+<span class="string">&quot;/&quot;</span>+bs[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> (<span class="number">1</span>):</span><br><span class="line">            suiji = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(所有的文件夹)-<span class="number">1</span>)  <span class="comment"># 生成1到100之间的随机整数</span></span><br><span class="line">            suiji !=random_integer</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment">#print(所有的文件夹[suiji])</span></span><br><span class="line">        bs=os.listdir(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span> +所有的文件夹[suiji])</span><br><span class="line">        random.shuffle(bs)</span><br><span class="line">        a=cv2.imread(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span> +所有的文件夹[suiji]+<span class="string">&quot;/&quot;</span>+bs[<span class="number">0</span>])</span><br><span class="line">        resized_image = cv2.resize(a, (<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line">        img1.append(resized_image)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">r&quot;LSWL\datalist\test\\&quot;</span> +所有的文件夹[suiji]+<span class="string">&quot;/&quot;</span>+bs[<span class="number">0</span>])</span><br><span class="line">        lab.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">model=resnet50网络<span class="number">1.</span>resnet50(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;当前设备是:&#x27;</span>, device)</span><br><span class="line">model.to(device)</span><br><span class="line">model_path = <span class="string">r&#x27;1.0483641744940542e-05.pth&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.path.exists(model_path):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;开始加载模型&#x27;</span>)</span><br><span class="line">    model.load_state_dict(torch.load(model_path))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;没有找到模型，退出！&#x27;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 测试模式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    img2=[]</span><br><span class="line">    img3=[]</span><br><span class="line">    lab1=[]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):</span><br><span class="line">        img_pil = Image.fromarray(cv2.cvtColor(img0[j],cv2.COLOR_BGR2RGB))</span><br><span class="line">        img_pil = toTensor(img_pil)</span><br><span class="line">        img_pil=transforms.RandomHorizontalFlip()(img_pil)     <span class="comment"># 50%的概率进行随机水平翻转</span></span><br><span class="line">        img_pil=transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0</span>)(img_pil)</span><br><span class="line">        img_pil= transforms.Normalize(mean = [ <span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span> ], std = [ <span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span> ])(img_pil) <span class="comment">#处理数据【-1，1】之间</span></span><br><span class="line">        img2.append(img_pil)</span><br><span class="line">    </span><br><span class="line">        img_pil = Image.fromarray(cv2.cvtColor(img1[j],cv2.COLOR_BGR2RGB))</span><br><span class="line">        img_pil = toTensor(img_pil)</span><br><span class="line">        img_pil=transforms.RandomHorizontalFlip()(img_pil)     <span class="comment"># 50%的概率进行随机水平翻转</span></span><br><span class="line">        img_pil=transforms.ColorJitter(brightness=<span class="number">0.2</span>, contrast=<span class="number">0.2</span>, saturation=<span class="number">0.2</span>, hue=<span class="number">0</span>)(img_pil)</span><br><span class="line">        img_pil= transforms.Normalize(mean = [ <span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span> ], std = [ <span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span> ])(img_pil) <span class="comment">#处理数据【-1，1】之间</span></span><br><span class="line">        img3.append(img_pil)</span><br><span class="line">        lab1.append(lab[j])</span><br><span class="line"></span><br><span class="line">    a = torch.stack(img2, dim=<span class="number">0</span>)</span><br><span class="line">    b = torch.stack(img3, dim=<span class="number">0</span>)</span><br><span class="line">    lab=torch.tensor(lab)</span><br><span class="line">    a, b,lab= Variable(a).cuda(), Variable(b).cuda(), Variable(lab).cuda()</span><br><span class="line">    组合=model(a,b)</span><br><span class="line">    outputs = model(a,b)</span><br><span class="line">    <span class="comment"># 将数据张量转换为 numpy 数组</span></span><br><span class="line">    array = outputs.cpu().detach().numpy()</span><br><span class="line">    array=(array &gt; <span class="number">0.5</span>)</span><br><span class="line">    ants_values = array[<span class="number">0</span>]</span><br><span class="line">    ants_valuess=ants_values[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(ants_valuess,lab1)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="余弦算法"><a href="#余弦算法" class="headerlink" title="余弦算法"></a>余弦算法</h3><p>最后我们拓展一个算法，余弦算法，也叫余弦相似度。</p>
<p>我们先来讲用法。</p>
<p>假设有2个矩阵：<code>[[1, 3], [5, 7]] 和 [[2, 4], [6, 8]]</code>, 求2者的余弦相似。</p>
<p>先按照<strong>dim&#x3D;0</strong>来求余弦相似度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">input1 = torch.tensor([[<span class="number">1</span>, <span class="number">3</span>], [<span class="number">5</span>, <span class="number">7</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">input2 = torch.tensor([[<span class="number">2</span>, <span class="number">4</span>], [<span class="number">6</span>, <span class="number">8</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">output = nn.functional.cosine_similarity(input1, input2, dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.9923, 0.9983])</span><br></pre></td></tr></table></figure>

<p>当在PyTorch中使用<code>nn.functional.cosine_similarity</code>函数时，设置<code>dim=0</code>表示沿着张量的第一个维度进行相似性计算。这意味着对输入张量中不同样本之间的相似性进行比较。</p>
<p>举例来说，假设有一个形状为<code>(N, D)</code>的输入张量，其中<code>N</code>表示样本数量，<code>D</code>表示特征维度。当设置<code>dim=0</code>时，将会计算每一对样本之间的余弦相似度，并返回一个大小为<code>(N, N)</code>的相似性矩阵。该矩阵中的每个元素(i, j)代表了样本i和样本j之间的余弦相似度。</p>
<p>因此，在设置<code>dim=0</code>时，PyTorch会沿着第一个维度计算相似性，并返回适合多个样本比较的结果。这有助于我们理解不同样本之间的关系以及它们在特征空间中的位置关系。</p>
<p>余弦算法计算文本相似度</p>
<p>举个例子：</p>
<p>句子A: 他喜欢吃香蕉和苹果。</p>
<p>句子B: 她喜欢吃苹果和香蕉。</p>
<p>首先，我们进行分词：</p>
<p>句子A: 他&#x2F;喜欢&#x2F;吃&#x2F;香蕉&#x2F;和&#x2F;苹果</p>
<p>句子B: 她&#x2F;喜欢&#x2F;吃&#x2F;苹果&#x2F;和&#x2F;香蕉</p>
<p>然后，列出所有的词：</p>
<p>他，喜欢，吃，香蕉，和，苹果</p>
<p>她，喜欢，吃，苹果，和，香蕉</p>
<p>接下来是计算词频：</p>
<p>句子A: 他1，喜欢1, 吃1, 香蕉1, 和1, 苹果1</p>
<p>句子B: 她1, 喜欢1, 吃1, 苹果1, 和1, 香蕉1</p>
<p>最后得到词频向量：</p>
<p>句子A：(1, 1, 1, 1, 1, 1)</p>
<p>句子B：(1, 1, 1, 0, 1, 0)</p>
<p>通过公式</p>
<p><img src="C:/Users/86150/Documents/WeChat%2520Files/wxid_1tzda7negup922/FileStorage/File/2024-01/%25E4%25BD%2599%25E5%25BC%25A6%25E7%259B%25B8%25E4%25BC%25BC%25E5%25BA%25A6%25E5%2585%25AC%25E5%25BC%258F.png" alt="余弦相似度公式"></p>
<p>可以求得结果为0.78</p>
<p>大家可以计算一下</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://Hello123Candy.github.io">星想事成</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://hello123candy.github.io/2024/01/12/%E6%9B%B9%E7%84%B1%E6%9D%B0%E7%9A%84%E7%AC%94%E8%AE%B0/">http://hello123candy.github.io/2024/01/12/%E6%9B%B9%E7%84%B1%E6%9D%B0%E7%9A%84%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://Hello123Candy.github.io" target="_blank">星想事成</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/./picture/33.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/01/12/2023/" title="2023"><img class="cover" src="/./picture/34.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">2023</div></div></a></div><div class="next-post pull-right"><a href="/2024/01/11/Gradio(1)/" title="Gradio(1)"><img class="cover" src="/./picture/32.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Gradio(1)</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/7.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">星想事成</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">41</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xxxxx" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:xxxxxx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome to my blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9B%B9%E7%84%B1%E6%9D%B0%E7%9A%84%E7%AC%94%E8%AE%B0"><span class="toc-number">1.</span> <span class="toc-text">曹焱杰的笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%89%8B%E5%86%99%E7%AE%80%E5%8D%95%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.</span> <span class="toc-text">一、手写简单的卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Convolution-Layers%EF%BC%88%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%89"><span class="toc-number">1.1.0.1.</span> <span class="toc-text">Convolution Layers（卷积层）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Pooling-layers%EF%BC%88%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%89"><span class="toc-number">1.1.0.2.</span> <span class="toc-text">Pooling layers（池化层）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Non-linear-Activations-weighted-sum-nonlinearity%EF%BC%89%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.0.3.</span> <span class="toc-text">Non-linear Activations (weighted sum, nonlinearity）非线性函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Linear-Layers%EF%BC%88%E7%BA%BF%E6%80%A7%E5%B1%82%EF%BC%89"><span class="toc-number">1.1.0.4.</span> <span class="toc-text">Linear Layers（线性层）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.0.5.</span> <span class="toc-text">手动实现一个卷积神经网络</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E4%BA%8C%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.</span> <span class="toc-text">二、实现卷积网络二分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.2.0.1.</span> <span class="toc-text">损失函数和反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-number">1.2.0.1.1.</span> <span class="toc-text">为什么使用损失函数？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-number">1.2.0.1.2.</span> <span class="toc-text">如何选择损失函数？</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.2.0.2.</span> <span class="toc-text">优化器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.2.0.3.</span> <span class="toc-text">二分类的实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%A6%82%E4%BD%95%E4%BF%9D%E5%AD%98"><span class="toc-number">1.2.0.4.</span> <span class="toc-text">模型如何保存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">1.2.0.5.</span> <span class="toc-text">测试</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="toc-number">1.3.</span> <span class="toc-text">三、实现卷积网络多分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.3.0.1.</span> <span class="toc-text">输出层的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.3.0.2.</span> <span class="toc-text">激活函数的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.3.0.3.</span> <span class="toc-text">损失函数的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%A4%9A%E5%88%86%E7%B1%BB%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.3.0.4.</span> <span class="toc-text">卷积多分类的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.0.4.1.</span> <span class="toc-text">训练模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.0.4.2.</span> <span class="toc-text">测试模型</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%AA%E7%94%9F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.4.</span> <span class="toc-text">孪生网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%AA%E7%94%9F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B"><span class="toc-number">1.4.1.</span> <span class="toc-text">孪生网络简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%AA%E7%94%9F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.4.2.</span> <span class="toc-text">孪生网络的损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E6%89%8B%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%AD%AA%E7%94%9F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.4.3.</span> <span class="toc-text">动手实现一个孪生网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">行人重识别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.3.2.</span> <span class="toc-text">微调模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%84%E7%90%86"><span class="toc-number">1.4.3.3.</span> <span class="toc-text">数据集处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">1.4.3.4.</span> <span class="toc-text">模型训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%AA%E7%94%9F%E7%BD%91%E7%BB%9C%E6%B5%8B%E8%AF%95"><span class="toc-number">1.4.4.</span> <span class="toc-text">孪生网络测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%99%E5%BC%A6%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.5.</span> <span class="toc-text">余弦算法</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/02/29/html%E8%A1%A8%E6%A0%BC%E6%A0%87%E7%AD%BE&amp;%E5%88%97%E8%A1%A8%E6%A0%87%E7%AD%BE/" title="html表格标签&amp;列表标签"><img src="/./picture/a9.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="html表格标签&amp;列表标签"/></a><div class="content"><a class="title" href="/2024/02/29/html%E8%A1%A8%E6%A0%BC%E6%A0%87%E7%AD%BE&amp;%E5%88%97%E8%A1%A8%E6%A0%87%E7%AD%BE/" title="html表格标签&amp;列表标签">html表格标签&amp;列表标签</a><time datetime="2024-02-29T11:44:29.000Z" title="发表于 2024-02-29 19:44:29">2024-02-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/29/html%E6%A0%87%E7%AD%BE/" title="html标签"><img src="/./picture/a9.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="html标签"/></a><div class="content"><a class="title" href="/2024/02/29/html%E6%A0%87%E7%AD%BE/" title="html标签">html标签</a><time datetime="2024-02-29T11:28:17.000Z" title="发表于 2024-02-29 19:28:17">2024-02-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/01/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/" title="计算机组成原理"><img src="/./picture/36.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机组成原理"/></a><div class="content"><a class="title" href="/2024/01/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/" title="计算机组成原理">计算机组成原理</a><time datetime="2024-01-15T11:56:16.000Z" title="发表于 2024-01-15 19:56:16">2024-01-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/01/12/flask/" title="flask"><img src="/./picture/35.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="flask"/></a><div class="content"><a class="title" href="/2024/01/12/flask/" title="flask">flask</a><time datetime="2024-01-12T09:43:31.000Z" title="发表于 2024-01-12 17:43:31">2024-01-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/01/12/2023/" title="2023"><img src="/./picture/34.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2023"/></a><div class="content"><a class="title" href="/2024/01/12/2023/" title="2023">2023</a><time datetime="2024-01-12T07:36:07.000Z" title="发表于 2024-01-12 15:36:07">2024-01-12</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/./picture/33.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By 星想事成</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>